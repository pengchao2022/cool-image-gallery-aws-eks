name: Deploy Comic Website

on:
  push:
    branches: [main]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: comic-website-prod
  PROJECT_NAME: comic-website
  ENVIRONMENT: prod
  NAMESPACE: comic-website
  TF_BACKEND_BUCKET: comic-website-tfstate-2024

jobs:
  terraform-infrastructure:
    name: "Terraform - Create Infrastructure"
    runs-on: ubuntu-latest
    environment: production
    outputs:
      backend_ecr_url: ${{ steps.terraform-outputs.outputs.backend_ecr_url }}
      frontend_ecr_url: ${{ steps.terraform-outputs.outputs.frontend_ecr_url }}
      rds_endpoint: ${{ steps.terraform-outputs.outputs.rds_endpoint }}
      rds_port: ${{ steps.terraform-outputs.outputs.rds_port }}
      rds_username: ${{ steps.terraform-outputs.outputs.rds_username }}
      rds_database: ${{ steps.terraform-outputs.outputs.rds_database }}
      s3_bucket_name: ${{ steps.terraform-outputs.outputs.s3_bucket_name }}
      s3_bucket_region: ${{ steps.terraform-outputs.outputs.s3_bucket_region }}
      s3_bucket_arn: ${{ steps.terraform-outputs.outputs.s3_bucket_arn }}
      alb_controller_role_arn: ${{ steps.terraform-outputs.outputs.alb_controller_role_arn }}
      backend_role_arn: ${{ steps.terraform-outputs.outputs.backend_role_arn }}
      frontend_role_arn: ${{ steps.terraform-outputs.outputs.frontend_role_arn }}

    steps:
      - name: ğŸ§© Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”‘ Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: ğŸ— Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.0

      - name: âš™ï¸ Terraform Init
        run: |
          cd terraform
          terraform init \
            -backend-config="bucket=${{ env.TF_BACKEND_BUCKET }}" \
            -backend-config="key=terraform/state/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="encrypt=true" \
            -backend-config="dynamodb_table=comic-website-tfstate-lock"

      - name: âœ… Terraform Validate
        run: |
          cd terraform
          terraform validate

      - name: ğŸš€ Terraform Plan and Apply
        run: |
          cd terraform
          terraform plan -out=tfplan \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="project_name=${{ env.PROJECT_NAME }}" \
            -var="environment=${{ env.ENVIRONMENT }}" \
            -var="db_password=${{ secrets.DB_PASSWORD }}"
          terraform apply -auto-approve tfplan

      - name: ğŸ” Debug Terraform State
        run: |
          cd terraform
          echo "ğŸ” Debugging Terraform state..."
          
          # æ£€æŸ¥æ‰€æœ‰èµ„æº
          echo "=== All Terraform Resources ==="
          terraform state list
          
          # æ£€æŸ¥ ALB Controller ç›¸å…³èµ„æº
          echo "=== ALB Controller Resources ==="
          terraform state list | grep -E "(alb_controller|aws_iam_role)" || echo "No ALB controller resources found"
          
          # æ£€æŸ¥ outputs.tf æ–‡ä»¶
          echo "=== Outputs.tf Content ==="
          cat outputs.tf

      - name: ğŸ“‹ Get Terraform Outputs
        id: terraform-outputs
        run: |
          cd terraform
          echo "ğŸ” Getting Terraform outputs..."
          
          terraform output -json > outputs.json
          
          BACKEND_ECR_URL=$(jq -r '.backend_repository_url.value // empty' outputs.json)
          FRONTEND_ECR_URL=$(jq -r '.frontend_repository_url.value // empty' outputs.json)
          RDS_ENDPOINT=$(jq -r '.rds_endpoint.value // empty' outputs.json)
          RDS_PORT=$(jq -r '.rds_port.value // "5432"' outputs.json)
          RDS_USERNAME=$(jq -r '.rds_username.value // "comicadmin"' outputs.json)
          RDS_DATABASE=$(jq -r '.rds_database_name.value // "comicdb"' outputs.json)
          S3_BUCKET_NAME=$(jq -r '.s3_bucket_name.value // empty' outputs.json)
          S3_BUCKET_REGION=$(jq -r '.s3_bucket_region.value // empty' outputs.json)
          S3_BUCKET_ARN=$(jq -r '.s3_bucket_arn.value // empty' outputs.json)
          ALB_CONTROLLER_ROLE_ARN=$(jq -r '.alb_controller_role_arn.value // empty' outputs.json)
          BACKEND_ROLE_ARN=$(jq -r '.backend_role_arn.value // empty' outputs.json)
          FRONTEND_ROLE_ARN=$(jq -r '.frontend_role_arn.value // empty' outputs.json)
          
          echo "Backend ECR URL: $BACKEND_ECR_URL"
          echo "Frontend ECR URL: $FRONTEND_ECR_URL"
          echo "RDS Endpoint: $RDS_ENDPOINT"
          echo "RDS Port: $RDS_PORT"
          echo "RDS Username: $RDS_USERNAME"
          echo "RDS Database: $RDS_DATABASE"
          echo "S3 Bucket Name: $S3_BUCKET_NAME"
          echo "S3 Bucket Region: $S3_BUCKET_REGION"
          echo "ALB Controller Role ARN: $ALB_CONTROLLER_ROLE_ARN"
          echo "Backend Role ARN: $BACKEND_ROLE_ARN"
          echo "Frontend Role ARN: $FRONTEND_ROLE_ARN"
          
          # å¦‚æœ ALB Controller Role ARN ä¸ºç©ºï¼Œå°è¯•ä» IAM è·å–
          if [ -z "$ALB_CONTROLLER_ROLE_ARN" ]; then
            echo "âš ï¸ ALB Controller Role ARN is empty from Terraform outputs"
            echo "ğŸ” Trying to find ALB Controller role from IAM..."
            
            # æŸ¥æ‰¾å¯èƒ½çš„ ALB Controller role
            ALB_ROLE=$(aws iam list-roles --query 'Roles[?contains(RoleName, `alb-controller`) || contains(RoleName, `alb_controller`) || contains(RoleName, `ALB`)].RoleName' --output text)
            if [ -n "$ALB_ROLE" ]; then
              ALB_CONTROLLER_ROLE_ARN="arn:aws:iam::319998871902:role/$ALB_ROLE"
              echo "âœ… Found ALB Controller role: $ALB_CONTROLLER_ROLE_ARN"
            else
              # ä½¿ç”¨é»˜è®¤çš„ role åç§°
              ALB_CONTROLLER_ROLE_ARN="arn:aws:iam::319998871902:role/comic-website-prod-alb-controller"
              echo "âš ï¸ Using default ALB Controller role: $ALB_CONTROLLER_ROLE_ARN"
            fi
          fi
          
          if [ -z "$BACKEND_ECR_URL" ]; then
            echo "âŒ Backend ECR URL is empty"
            exit 1
          fi
          
          if [ -z "$FRONTEND_ECR_URL" ]; then
            echo "âŒ Frontend ECR URL is empty"
            exit 1
          fi
          
          if [ -z "$RDS_ENDPOINT" ]; then
            echo "âŒ RDS Endpoint is empty"
            exit 1
          fi
          
          if [ -z "$S3_BUCKET_NAME" ]; then
            echo "âŒ S3 Bucket Name is empty"
            exit 1
          fi
          
          if [ -z "$BACKEND_ROLE_ARN" ]; then
            echo "âš ï¸ Backend Role ARN is empty - backend may not have AWS permissions"
          fi
          
          if [ -z "$FRONTEND_ROLE_ARN" ]; then
            echo "âš ï¸ Frontend Role ARN is empty - frontend may not have AWS permissions"
          fi
          
          echo "backend_ecr_url=$BACKEND_ECR_URL" >> $GITHUB_OUTPUT
          echo "frontend_ecr_url=$FRONTEND_ECR_URL" >> $GITHUB_OUTPUT
          echo "rds_endpoint=$RDS_ENDPOINT" >> $GITHUB_OUTPUT
          echo "rds_port=$RDS_PORT" >> $GITHUB_OUTPUT
          echo "rds_username=$RDS_USERNAME" >> $GITHUB_OUTPUT
          echo "rds_database=$RDS_DATABASE" >> $GITHUB_OUTPUT
          echo "s3_bucket_name=$S3_BUCKET_NAME" >> $GITHUB_OUTPUT
          echo "s3_bucket_region=$S3_BUCKET_REGION" >> $GITHUB_OUTPUT
          echo "s3_bucket_arn=$S3_BUCKET_ARN" >> $GITHUB_OUTPUT
          echo "alb_controller_role_arn=$ALB_CONTROLLER_ROLE_ARN" >> $GITHUB_OUTPUT
          echo "backend_role_arn=$BACKEND_ROLE_ARN" >> $GITHUB_OUTPUT
          echo "frontend_role_arn=$FRONTEND_ROLE_ARN" >> $GITHUB_OUTPUT
          echo "âœ… All outputs extracted successfully"

      - name: ğŸ” Verify IAM Role Configuration
        run: |
          echo "ğŸ” Verifying IAM role configuration..."
          
          ALB_ROLE_ARN="${{ steps.terraform-outputs.outputs.alb_controller_role_arn }}"
          ROLE_NAME=$(echo $ALB_ROLE_ARN | cut -d'/' -f2)
          
          echo "ALB Role ARN: $ALB_ROLE_ARN"
          echo "Role Name: $ROLE_NAME"
          
          # æ£€æŸ¥ IAM role æ˜¯å¦å­˜åœ¨
          if aws iam get-role --role-name "$ROLE_NAME" &> /dev/null; then
            echo "âœ… IAM role exists: $ROLE_NAME"
            
            # æ£€æŸ¥ä¿¡ä»»ç­–ç•¥
            echo "=== IAM Role Trust Policy ==="
            aws iam get-role --role-name "$ROLE_NAME" --query 'Role.AssumeRolePolicyDocument.Statement[]'
            
            # æ£€æŸ¥é™„åŠ çš„ç­–ç•¥
            echo "=== Attached Policies ==="
            aws iam list-attached-role-policies --role-name "$ROLE_NAME" --query 'AttachedPolicies[]'
          else
            echo "âŒ IAM role does not exist: $ROLE_NAME"
            echo "ğŸ” Available roles with 'alb' in name:"
            aws iam list-roles --query 'Roles[?contains(RoleName, `alb`)].RoleName' --output table
          fi

      - name: ğŸ” Verify RDS Configuration
        run: |
          echo "ğŸ” Verifying RDS configuration..."
          aws rds describe-db-instances \
            --region ${{ env.AWS_REGION }} \
            --query 'DBInstances[].{DBInstanceIdentifier:DBInstanceIdentifier, Status:DBInstanceStatus, Endpoint:Endpoint.Address, Port:Endpoint.Port}' \
            --output table

      - name: ğŸ” Verify S3 Configuration
        run: |
          echo "ğŸ” Verifying S3 configuration..."
          aws s3api list-buckets \
            --region ${{ env.AWS_REGION }} \
            --query 'Buckets[].{Name:Name, CreationDate:CreationDate}' \
            --output table

      - name: â˜¸ï¸ Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: ğŸ§° Setup kubectl
        run: |
          sudo snap install kubectl --classic

  build-and-push-images:
    name: "Build and Push Docker Images"
    runs-on: ubuntu-latest
    needs: terraform-infrastructure
    environment: production
    if: needs.terraform-infrastructure.result == 'success'
    
    outputs:
      backend_image: ${{ steps.build-backend.outputs.image_tag }}
      frontend_image: ${{ steps.build-frontend.outputs.image_tag }}

    steps:
      - name: ğŸ§© Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: ğŸ”‘ Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: ğŸ” Get AWS Account ID and ECR Registry
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "ACCOUNT_ID=$ACCOUNT_ID" >> $GITHUB_ENV
          ECR_REGISTRY="$ACCOUNT_ID.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com"
          echo "ECR_REGISTRY=$ECR_REGISTRY" >> $GITHUB_ENV

      - name: ğŸ” Login to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin $ECR_REGISTRY

      - name: ğŸ” Set ECR URLs from previous job
        run: |
          echo "BACKEND_ECR_URL=${{ needs.terraform-infrastructure.outputs.backend_ecr_url }}" >> $GITHUB_ENV
          echo "FRONTEND_ECR_URL=${{ needs.terraform-infrastructure.outputs.frontend_ecr_url }}" >> $GITHUB_ENV

      - name: ğŸ— Check if Backend Image Exists
        id: check-backend
        run: |
          IMAGE_TAG="${GITHUB_SHA:0:8}"
          BACKEND_IMAGE="$BACKEND_ECR_URL:$IMAGE_TAG"
          if docker manifest inspect $BACKEND_IMAGE > /dev/null 2>&1; then
            echo "image_exists=true" >> $GITHUB_OUTPUT
            echo "image_tag=$BACKEND_IMAGE" >> $GITHUB_OUTPUT
          else
            echo "image_exists=false" >> $GITHUB_OUTPUT
            echo "image_tag=$BACKEND_IMAGE" >> $GITHUB_OUTPUT
          fi

      - name: ğŸ§± Build Backend Image (if needed)
        id: build-backend
        if: steps.check-backend.outputs.image_exists == 'false'
        run: |
          cd backend
          IMAGE_TAG="${GITHUB_SHA:0:8}"
          BACKEND_IMAGE="$BACKEND_ECR_URL:$IMAGE_TAG"
          
          echo "ğŸ” Checking file structure..."
          ls -la
          
          # ç¡®ä¿ scripts ç›®å½•å­˜åœ¨
          if [ ! -d "scripts" ]; then
            echo "ğŸ“ Creating scripts directory..."
            mkdir -p scripts
          fi
          
          # åˆ›å»ºè¿ç§»è„šæœ¬
          echo "ğŸ“ Creating migration script..."
          cat > scripts/migrate.js << 'EOF'
          import pg from 'pg';
          import dotenv from 'dotenv';

          // åŠ è½½ç¯å¢ƒå˜é‡
          dotenv.config();

          const { Client } = pg;

          // æ•°æ®åº“è¿æ¥é…ç½®
          const dbConfig = {
            host: process.env.RDS_HOST || process.env.DB_HOST,
            port: process.env.RDS_PORT || process.env.DB_PORT || 5432,
            user: process.env.RDS_USERNAME || process.env.DB_USERNAME,
            password: process.env.RDS_PASSWORD || process.env.DB_PASSWORD,
            database: process.env.RDS_DATABASE || process.env.DB_NAME,
            ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false,
          };

          // è¿ç§» SQL è¯­å¥
          const migrationSQL = `
          -- Enable trigram extension for better text search
          CREATE EXTENSION IF NOT EXISTS pg_trgm;

          -- Create users table
          CREATE TABLE IF NOT EXISTS users (
              id SERIAL PRIMARY KEY,
              username VARCHAR(50) UNIQUE NOT NULL,
              email VARCHAR(255) UNIQUE NOT NULL,
              password_hash VARCHAR(255) NOT NULL,
              created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
              updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
          );

          -- Create comics table
          CREATE TABLE IF NOT EXISTS comics (
              id SERIAL PRIMARY KEY,
              title VARCHAR(255) NOT NULL,
              description TEXT,
              tags VARCHAR(500),
              user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
              image_urls TEXT[] NOT NULL,
              created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
              updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
          );

          -- Create indexes
          CREATE INDEX IF NOT EXISTS idx_comics_user_id ON comics(user_id);
          CREATE INDEX IF NOT EXISTS idx_comics_created_at ON comics(created_at DESC);
          CREATE INDEX IF NOT EXISTS idx_comics_tags ON comics USING gin(tags gin_trgm_ops);
          CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);

          -- Create updated_at trigger function
          CREATE OR REPLACE FUNCTION update_updated_at_column()
          RETURNS TRIGGER AS $$
          BEGIN
              NEW.updated_at = CURRENT_TIMESTAMP;
              RETURN NEW;
          END;
          $$ language 'plpgsql';

          -- Create triggers for updated_at
          DROP TRIGGER IF EXISTS update_users_updated_at ON users;
          CREATE TRIGGER update_users_updated_at
              BEFORE UPDATE ON users
              FOR EACH ROW
              EXECUTE FUNCTION update_updated_at_column();

          DROP TRIGGER IF EXISTS update_comics_updated_at ON comics;
          CREATE TRIGGER update_comics_updated_at
              BEFORE UPDATE ON comics
              FOR EACH ROW
              EXECUTE FUNCTION update_updated_at_column();
          `;

          async function runMigration() {
            console.log('ğŸš€ Starting database migration...');
            console.log('ğŸ“Š Database configuration:');
            console.log(`   Host: ${dbConfig.host}`);
            console.log(`   Port: ${dbConfig.port}`);
            console.log(`   Database: ${dbConfig.database}`);
            console.log(`   User: ${dbConfig.user}`);
            console.log(`   Environment: ${process.env.NODE_ENV || 'development'}`);

            const client = new Client(dbConfig);

            try {
              // è¿æ¥æ•°æ®åº“
              console.log('ğŸ”— Connecting to database...');
              await client.connect();
              console.log('âœ… Connected to database successfully');

              // æ‰§è¡Œè¿ç§»
              console.log('ğŸ—ƒ Running migration SQL...');
              await client.query(migrationSQL);
              console.log('âœ… Migration completed successfully');

              // éªŒè¯è¡¨åˆ›å»º
              console.log('ğŸ” Verifying table creation...');
              const tablesResult = await client.query(`
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_type = 'BASE TABLE'
                ORDER BY table_name;
              `);

              console.log('ğŸ“‹ Created tables:');
              tablesResult.rows.forEach(row => {
                console.log(`   - ${row.table_name}`);
              });

              // éªŒè¯æ‰©å±•
              const extensionsResult = await client.query(`
                SELECT extname 
                FROM pg_extension 
                WHERE extname = 'pg_trgm';
              `);

              if (extensionsResult.rows.length > 0) {
                console.log('âœ… pg_trgm extension enabled');
              } else {
                console.log('âŒ pg_trgm extension not found');
              }

            } catch (error) {
              console.error('âŒ Migration failed:', error.message);
              console.error('Error details:', error);
              process.exit(1);
            } finally {
              await client.end();
              console.log('ğŸ”’ Database connection closed');
            }
          }

          // å¦‚æœæ˜¯ç›´æ¥è¿è¡Œæ­¤è„šæœ¬
          if (import.meta.url === `file://${process.argv[1]}`) {
            runMigration();
          }
          EOF

          echo "âœ… Migration script created"
          docker build -t $BACKEND_IMAGE .
          docker push $BACKEND_IMAGE
          echo "image_tag=$BACKEND_IMAGE" >> $GITHUB_OUTPUT

      - name: ğŸ¨ Build Frontend Image
        id: build-frontend
        run: |
          cd frontend
          IMAGE_TAG="${GITHUB_SHA:0:8}"
          FRONTEND_IMAGE="$FRONTEND_ECR_URL:$IMAGE_TAG"
          docker build -t $FRONTEND_IMAGE .
          docker push $FRONTEND_IMAGE
          echo "image_tag=$FRONTEND_IMAGE" >> $GITHUB_OUTPUT

  deploy-applications:
    name: "Deploy Applications to EKS"
    runs-on: ubuntu-latest
    needs: 
      - terraform-infrastructure
      - build-and-push-images
    environment: production
    if: needs.terraform-infrastructure.result == 'success'

    steps:
      - name: ğŸ§© Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”‘ Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: ğŸ” Debug Job Outputs
        run: |
          echo "ğŸ” Debugging job outputs..."
          echo "terraform-infrastructure outputs:"
          echo "  backend_ecr_url: ${{ needs.terraform-infrastructure.outputs.backend_ecr_url }}"
          echo "  frontend_ecr_url: ${{ needs.terraform-infrastructure.outputs.frontend_ecr_url }}"
          echo "  rds_endpoint: ${{ needs.terraform-infrastructure.outputs.rds_endpoint }}"
          echo "  rds_port: ${{ needs.terraform-infrastructure.outputs.rds_port }}"
          echo "  rds_username: ${{ needs.terraform-infrastructure.outputs.rds_username }}"
          echo "  rds_database: ${{ needs.terraform-infrastructure.outputs.rds_database }}"
          echo "  s3_bucket_name: ${{ needs.terraform-infrastructure.outputs.s3_bucket_name }}"
          echo "  s3_bucket_region: ${{ needs.terraform-infrastructure.outputs.s3_bucket_region }}"
          echo "  alb_controller_role_arn: ${{ needs.terraform-infrastructure.outputs.alb_controller_role_arn }}"
          echo "  backend_role_arn: ${{ needs.terraform-infrastructure.outputs.backend_role_arn }}"
          echo "  frontend_role_arn: ${{ needs.terraform-infrastructure.outputs.frontend_role_arn }}"
          
          echo "build-and-push-images outputs:"
          echo "  backend_image: ${{ needs.build-and-push-images.outputs.backend_image }}"
          echo "  frontend_image: ${{ needs.build-and-push-images.outputs.frontend_image }}"

      - name: â˜¸ï¸ Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: ğŸ§° Setup kubectl
        run: |
          sudo snap install kubectl --classic

      - name: ğŸ”§ Install yq for YAML processing
        run: |
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod a+x /usr/local/bin/yq

      - name: ğŸ”§ Install envsubst
        run: |
          sudo apt-get update
          sudo apt-get install -y gettext-base

      - name: ğŸ”§ Update Service Account Role ARNs
        run: |
          echo "ğŸ”§ Updating Service Account Role ARNs in YAML files..."
          
          ALB_ROLE_ARN="${{ needs.terraform-infrastructure.outputs.alb_controller_role_arn }}"
          BACKEND_ROLE_ARN="${{ needs.terraform-infrastructure.outputs.backend_role_arn }}"
          FRONTEND_ROLE_ARN="${{ needs.terraform-infrastructure.outputs.frontend_role_arn }}"
          
          echo "ALB Role ARN: $ALB_ROLE_ARN"
          echo "Backend Role ARN: $BACKEND_ROLE_ARN"
          echo "Frontend Role ARN: $FRONTEND_ROLE_ARN"
          
          # æ›´æ–° ALB Service Account - ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å ä½ç¬¦ ALB_ROLE_ARN_PLACEHOLDER
          if [ -n "$ALB_ROLE_ARN" ]; then
            sed -i "s|ALB_ROLE_ARN_PLACEHOLDER|$ALB_ROLE_ARN|g" k8s/service-accounts/alb-service-account.yaml
            echo "âœ… ALB Service Account updated with ARN: $ALB_ROLE_ARN"
          else
            echo "âŒ ALB Role ARN is empty"
            exit 1
          fi
          
          # æ›´æ–° Backend Service Account
          if [ -n "$BACKEND_ROLE_ARN" ]; then
            sed -i "s|arn:aws:iam::123456789012:role/comic-website-prod-app-backend-role|$BACKEND_ROLE_ARN|g" k8s/service-accounts/backend-service-account.yaml
            echo "âœ… Backend Service Account updated"
          else
            echo "âš ï¸ Backend Role ARN is empty - using existing value"
          fi
          
          # æ›´æ–° Frontend Service Account
          if [ -n "$FRONTEND_ROLE_ARN" ]; then
            sed -i "s|arn:aws:iam::123456789012:role/comic-website-prod-app-frontend-role|$FRONTEND_ROLE_ARN|g" k8s/service-accounts/frontend-service-account.yaml
            echo "âœ… Frontend Service Account updated"
          else
            echo "âš ï¸ Frontend Role ARN is empty - using existing value"
          fi
          
          echo "âœ… All Service Account Role ARNs updated"
          
          # éªŒè¯æ›¿æ¢ç»“æœ
          echo "=== Verification ==="
          echo "ALB Service Account ARN:"
          grep "eks.amazonaws.com/role-arn" k8s/service-accounts/alb-service-account.yaml

      - name: ğŸ”§ Apply ALB Controller ServiceAccount
        run: |
          echo "ğŸ”§ Applying ALB Controller ServiceAccount..."
          
          # åˆ é™¤ç°æœ‰çš„ ServiceAccountï¼ˆå¦‚æœå­˜åœ¨ï¼‰
          kubectl delete serviceaccount aws-load-balancer-controller -n kube-system --ignore-not-found=true
          
          # åº”ç”¨æ–°çš„ ServiceAccount
          kubectl apply -f k8s/service-accounts/alb-service-account.yaml
          
          # éªŒè¯ ServiceAccount
          echo "=== Verifying ALB ServiceAccount ==="
          kubectl get serviceaccount aws-load-balancer-controller -n kube-system -o yaml | grep -A 5 annotations

      - name: ğŸ”„ Restart ALB Controller
        run: |
          echo "ğŸ”„ Restarting ALB Controller to pick up new role..."
          kubectl rollout restart deployment/aws-load-balancer-controller -n kube-system
          
          echo "â³ Waiting for ALB Controller to be ready..."
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system --timeout=300s
          
          echo "âœ… ALB Controller restarted"
          
          # æ£€æŸ¥ ALB Controller çŠ¶æ€
          echo "=== ALB Controller Status ==="
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
          
          echo "=== ALB Controller Logs ==="
          # å¿½ç•¥æ—¥å¿—é”™è¯¯ï¼Œä½¿ç”¨ || true é˜²æ­¢è„šæœ¬é€€å‡º
          kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=10 || true

      - name: ğŸ—ƒ Create Namespace
        run: |
          kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -

      - name: ğŸ”§ Update Kubernetes Secrets with Real Values
        run: |
          echo "ğŸ”§ Updating Kubernetes secrets with real values..."
          
          # é‡æ–°è®¾ç½®ç¯å¢ƒå˜é‡ç”¨äº envsubst
          export RDS_ENDPOINT="${{ needs.terraform-infrastructure.outputs.rds_endpoint }}"
          export RDS_PORT="${{ needs.terraform-infrastructure.outputs.rds_port }}"
          export RDS_USERNAME="${{ needs.terraform-infrastructure.outputs.rds_username }}"
          export RDS_DATABASE="${{ needs.terraform-infrastructure.outputs.rds_database }}"
          export JWT_SECRET="${{ secrets.JWT_SECRET }}"
          export DATABASE_URL="postgresql://$RDS_USERNAME:${{ secrets.DB_PASSWORD }}@$RDS_ENDPOINT:$RDS_PORT/$RDS_DATABASE"
          export RDS_HOST=$(echo "$RDS_ENDPOINT" | cut -d':' -f1)
          export RDS_PASSWORD="${{ secrets.DB_PASSWORD }}"
          export S3_BUCKET_NAME="${{ needs.terraform-infrastructure.outputs.s3_bucket_name }}"
          export AWS_REGION="${{ env.AWS_REGION }}"
          
          echo "Environment variables set for substitution:"
          echo "RDS_HOST: $RDS_HOST"
          echo "RDS_DATABASE: $RDS_DATABASE" 
          echo "S3_BUCKET_NAME: $S3_BUCKET_NAME"
          echo "JWT_SECRET length: ${#JWT_SECRET} characters"
          
          # ä½¿ç”¨ envsubst å¤„ç†æ‰€æœ‰ Secret æ–‡ä»¶ï¼ˆåŸåœ°æ›¿æ¢ï¼‰
          echo "=== Processing Secret Files ==="
          
          # å¤„ç† rds-secret.yaml
          echo "Processing rds-secret.yaml"
          envsubst < k8s/configs/rds-secret.yaml > k8s/configs/rds-secret.yaml.tmp
          mv k8s/configs/rds-secret.yaml.tmp k8s/configs/rds-secret.yaml
          
          # å¤„ç† backend-secret.yaml
          echo "Processing backend-secret.yaml"
          envsubst < k8s/configs/backend-secret.yaml > k8s/configs/backend-secret.yaml.tmp
          mv k8s/configs/backend-secret.yaml.tmp k8s/configs/backend-secret.yaml
          
          # å¤„ç† s3-secret.yaml
          echo "Processing s3-secret.yaml"
          envsubst < k8s/configs/s3-secret.yaml > k8s/configs/s3-secret.yaml.tmp
          mv k8s/configs/s3-secret.yaml.tmp k8s/configs/s3-secret.yaml
          
          echo "âœ… All secret files processed with real values"

      - name: ğŸš€ Deploy using Kustomize
        run: |
          echo "ğŸš€ Applying all manifests using Kustomize..."
          cd k8s
          kubectl apply -k . --namespace ${{ env.NAMESPACE }}
          echo "âœ… Kustomize deployment completed"

      - name: ğŸ—ƒ Run Database Migrations
        run: |
          echo "ğŸ—ƒ Running database migrations..."
          
          BACKEND_IMAGE="${{ needs.build-and-push-images.outputs.backend_image }}"
          echo "Backend Image: $BACKEND_IMAGE"
          
          # ä½¿ç”¨ sed æ›¿æ¢é•œåƒå ä½ç¬¦
          sed "s|PLACEHOLDER_BACKEND_IMAGE|$BACKEND_IMAGE|g" k8s/migrations/db-migration-job.yaml > /tmp/db-migration-job.yaml
          
          echo "ğŸ”§ Applying migration Job..."
          kubectl apply -f /tmp/db-migration-job.yaml

          echo "â³ Waiting for migration to complete..."
          if kubectl wait --for=condition=complete job/db-migration -n ${{ env.NAMESPACE }} --timeout=300s; then
            echo "âœ… Database migration completed successfully"
            
            # è·å–è¿ç§»æ—¥å¿—
            echo "=== Migration Logs ==="
            MIGRATION_POD=$(kubectl get pods -n ${{ env.NAMESPACE }} -l job-name=db-migration -o jsonpath='{.items[0].metadata.name}')
            kubectl logs $MIGRATION_POD -n ${{ env.NAMESPACE }}
            
            # æ¸…ç†è¿ç§» Job
            kubectl delete job db-migration -n ${{ env.NAMESPACE }}
            echo "âœ… Migration Job cleaned up"
          else
            echo "âŒ Database migration failed or timed out"
            
            # è·å–å¤±è´¥æ—¥å¿—
            echo "=== Migration Logs (Failed) ==="
            MIGRATION_POD=$(kubectl get pods -n ${{ env.NAMESPACE }} -l job-name=db-migration -o jsonpath='{.items[0].metadata.name}')
            if [ -n "$MIGRATION_POD" ]; then
              kubectl logs $MIGRATION_POD -n ${{ env.NAMESPACE }}
            fi
            
            # è·å– Job è¯¦æƒ…
            echo "=== Job Details ==="
            kubectl describe job db-migration -n ${{ env.NAMESPACE }}
            
            echo "âš ï¸ Migration Job kept for debugging. Manual cleanup required."
            exit 1
          fi

      - name: ğŸ”„ Update Backend Image
        run: |
          echo "ğŸ”„ Updating backend image to: ${{ needs.build-and-push-images.outputs.backend_image }}"
          kubectl set image deployment/backend-deployment backend=${{ needs.build-and-push-images.outputs.backend_image }} -n ${{ env.NAMESPACE }}
          echo "âœ… Backend image updated"
          
          # ç­‰å¾…é•œåƒæ›´æ–°å®Œæˆ
          echo "â³ Waiting for backend image update to complete..."
          kubectl rollout status deployment/backend-deployment -n ${{ env.NAMESPACE }} --timeout=300s

      - name: ğŸ”„ Update Frontend Image
        run: |
          echo "ğŸ”„ Updating frontend image to: ${{ needs.build-and-push-images.outputs.frontend_image }}"
          kubectl set image deployment/frontend-deployment frontend=${{ needs.build-and-push-images.outputs.frontend_image }} -n ${{ env.NAMESPACE }}
          echo "âœ… Frontend image updated"

      - name: ğŸ” Verify Secrets Configuration
        run: |
          echo "ğŸ” Verifying secrets configuration..."
          sleep 5
          
          echo "=== RDS Secret Keys ==="
          kubectl get secret rds-secret -n ${{ env.NAMESPACE }} -o jsonpath='{.data}' | python3 -c "import json, sys; data = json.load(sys.stdin); print('Keys:', list(data.keys()))"
          
          echo "=== Backend Secret Keys ==="
          kubectl get secret backend-secret -n ${{ env.NAMESPACE }} -o jsonpath='{.data}' | python3 -c "import json, sys; data = json.load(sys.stdin); print('Keys:', list(data.keys()))"
          
          echo "=== S3 Secret Keys ==="
          kubectl get secret s3-secret -n ${{ env.NAMESPACE }} -o jsonpath='{.data}' | python3 -c "import json, sys; data = json.load(sys.stdin); print('Keys:', list(data.keys()))"

      - name: ğŸ” Verify Environment Variables Configuration
        run: |
          echo "ğŸ” Verifying environment variables configuration..."
          sleep 10
          
          # æ£€æŸ¥ deployment çš„ç¯å¢ƒå˜é‡é…ç½®
          echo "=== Deployment Environment Variables ==="
          kubectl get deployment backend-deployment -n ${{ env.NAMESPACE }} -o jsonpath='{.spec.template.spec.containers[0].env}' | python3 -c "import json, sys; data = json.load(sys.stdin); print(json.dumps(data, indent=2))"
          
          # ç­‰å¾… pod å¯åŠ¨å¹¶æ£€æŸ¥å®é™…ç¯å¢ƒå˜é‡
          echo "=== Waiting for pod to be ready ==="
          kubectl rollout status deployment/backend-deployment -n ${{ env.NAMESPACE }} --timeout=120s || true
          
          BACKEND_POD=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=backend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$BACKEND_POD" ]; then
            echo "=== Actual Pod Environment Variables ==="
            kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- env | grep -E "(DB_|NODE_ENV|PORT|JWT_|AWS_|S3_)" || echo "Pod not ready yet"
          else
            echo "âŒ No backend pod found"
          fi

      - name: â³ Wait for Deployments
        run: |
          echo "ğŸ” Waiting for deployments to be ready..."
          timeout 300s bash -c "
            while ! kubectl rollout status deployment/backend-deployment -n ${{ env.NAMESPACE }} --timeout=60s 2>/dev/null; do
              echo 'â³ Backend still deploying...'
              sleep 10
            done
          " || echo "âš ï¸ Backend deployment timeout, continuing..."

          timeout 180s bash -c "
            while ! kubectl rollout status deployment/frontend-deployment -n ${{ env.NAMESPACE }} --timeout=60s 2>/dev/null; do
              echo 'â³ Frontend still deploying...'
              sleep 10
            done
          " || echo "âš ï¸ Frontend deployment timeout, continuing..."

      - name: ğŸŒ Check ALB Controller Status
        run: |
          echo "ğŸŒ Checking ALB Controller status..."
          echo "=== ALB Controller Pods ==="
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
          
          echo "=== ALB Controller Logs ==="
          # å¿½ç•¥æ—¥å¿—é”™è¯¯
          kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=20 || true
          
          echo "=== Ingress Status ==="
          kubectl get ingress -n ${{ env.NAMESPACE }} -o wide

      - name: ğŸ“Š Final Deployment Status
        run: |
          echo "ğŸ“Š Final deployment status:"
          kubectl get pods -n ${{ env.NAMESPACE }}
          
          echo -e "\nğŸ“‹ Services:"
          kubectl get svc -n ${{ env.NAMESPACE }}
          
          echo -e "\nğŸŒ Ingress:"
          kubectl get ingress -n ${{ env.NAMESPACE }} -o yaml | grep -A 10 "status:"

      - name: ğŸ” Check Application Logs
        run: |
          echo "ğŸ” Checking backend logs..."
          BACKEND_POD=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=backend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$BACKEND_POD" ]; then
            echo "=== Backend Logs ==="
            kubectl logs $BACKEND_POD -n ${{ env.NAMESPACE }} --tail=50 || true
          else
            echo "âŒ No backend pod found"
          fi

      - name: ğŸŒ Get ALB URL
        run: |
          echo "ğŸš€ Retrieving ALB URL..."
          for i in {1..15}; do
            ALB_URL=$(kubectl get ingress comic-website-ingress -n ${{ env.NAMESPACE }} -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            if [ -n "$ALB_URL" ]; then
              echo "âœ… ALB is ready: http://$ALB_URL"
              echo "ALB_URL=http://$ALB_URL" >> $GITHUB_ENV
              break
            fi
            echo "â³ ALB not ready yet (attempt $i/15)..."
            sleep 20
          done
          
          if [ -z "$ALB_URL" ]; then
            echo "âŒ ALB creation failed after 15 attempts"
            echo "ğŸ” Checking ALB Controller logs for details..."

            kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=50 || true
            echo "ğŸ” Checking Ingress events..."
            kubectl describe ingress comic-website-ingress -n ${{ env.NAMESPACE }}
            exit 1
          fi