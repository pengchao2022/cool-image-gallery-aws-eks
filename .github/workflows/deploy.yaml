name: Deploy Comic Website

on:
  push:
    branches: [main]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: comic-website-prod
  PROJECT_NAME: comic-website
  ENVIRONMENT: prod
  NAMESPACE: comic-website
  TF_BACKEND_BUCKET: comic-website-tfstate-2024

jobs:
  terraform-infrastructure:
    name: "Terraform - Create Infrastructure"
    runs-on: ubuntu-latest
    environment: production
    outputs:
      backend_ecr_url: ${{ steps.terraform-outputs.outputs.backend_ecr_url }}
      frontend_ecr_url: ${{ steps.terraform-outputs.outputs.frontend_ecr_url }}
      community_ecr_url: ${{ steps.terraform-outputs.outputs.community_ecr_url }}
      rds_endpoint: ${{ steps.terraform-outputs.outputs.rds_endpoint }}
      rds_port: ${{ steps.terraform-outputs.outputs.rds_port }}
      rds_username: ${{ steps.terraform-outputs.outputs.rds_username }}
      rds_database: ${{ steps.terraform-outputs.outputs.rds_database }}
      s3_bucket_name: ${{ steps.terraform-outputs.outputs.s3_bucket_name }}
      s3_bucket_region: ${{ steps.terraform-outputs.outputs.s3_bucket_region }}
      alb_controller_role_arn: ${{ steps.terraform-outputs.outputs.alb_controller_role_arn }}
      backend_role_arn: ${{ steps.terraform-outputs.outputs.backend_role_arn }}
      frontend_role_arn: ${{ steps.terraform-outputs.outputs.frontend_role_arn }}
      community_role_arn: ${{ steps.terraform-outputs.outputs.community_role_arn }}
      alb_url: ${{ steps.terraform-outputs.outputs.alb_url }}
      # Redis é…ç½®è¾“å‡º
      redis_host: ${{ steps.terraform-outputs.outputs.redis_host }}
      redis_port: ${{ steps.terraform-outputs.outputs.redis_port }}
      # æ–°å¢ Community æ•°æ®åº“è¾“å‡º
      community_database_name: ${{ steps.terraform-outputs.outputs.community_database_name }}
      community_database_username: ${{ steps.terraform-outputs.outputs.community_database_username }}

    steps:
      - name: ğŸ§© Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”‘ Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: ğŸ— Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.0

      - name: âš™ï¸ Terraform Init
        run: |
          cd terraform
          terraform init \
            -backend-config="bucket=${{ env.TF_BACKEND_BUCKET }}" \
            -backend-config="key=terraform/state/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="encrypt=true" \
            -backend-config="dynamodb_table=comic-website-tfstate-lock"

      - name: âœ… Terraform Validate
        run: |
          cd terraform
          terraform validate

      - name: ğŸš€ Terraform Plan and Apply
        run: |
          cd terraform
          terraform plan -out=tfplan \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="project_name=${{ env.PROJECT_NAME }}" \
            -var="environment=${{ env.ENVIRONMENT }}" \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="community_db_password=${{ secrets.COMMUNITY_DB_PASSWORD }}" \
            -var="community_db_username=${{ secrets.COMMUNITY_DB_USERNAME }}" \
            -var="community_db_name=${{ secrets.COMMUNITY_DB_NAME }}"
          terraform apply -auto-approve tfplan

      - name: ğŸ“‹ Get Terraform Outputs
        id: terraform-outputs
        run: |
          cd terraform
          terraform output -json > outputs.json
          
          BACKEND_ECR_URL=$(jq -r '.backend_repository_url.value // empty' outputs.json)
          FRONTEND_ECR_URL=$(jq -r '.frontend_repository_url.value // empty' outputs.json)
          COMMUNITY_ECR_URL=$(jq -r '.community_repository_url.value // empty' outputs.json)
          RDS_ENDPOINT=$(jq -r '.rds_endpoint.value // empty' outputs.json)
          RDS_PORT=$(jq -r '.rds_port.value // "5432"' outputs.json)
          RDS_USERNAME=$(jq -r '.rds_username.value // "comicadmin"' outputs.json)
          RDS_DATABASE=$(jq -r '.rds_database_name.value // "comicdb"' outputs.json)
          S3_BUCKET_NAME=$(jq -r '.s3_bucket_name.value // empty' outputs.json)
          S3_BUCKET_REGION=$(jq -r '.s3_bucket_region.value // empty' outputs.json)
          ALB_CONTROLLER_ROLE_ARN=$(jq -r '.alb_controller_role_arn.value // empty' outputs.json)
          BACKEND_ROLE_ARN=$(jq -r '.backend_role_arn.value // empty' outputs.json)
          FRONTEND_ROLE_ARN=$(jq -r '.frontend_role_arn.value // empty' outputs.json)
          COMMUNITY_ROLE_ARN=$(jq -r '.community_role_arn.value // empty' outputs.json)
          ALB_URL=$(jq -r '.alb_url.value // empty' outputs.json)
          
          # Redis è¾“å‡º
          REDIS_HOST=$(jq -r '.redis_host.value // "redis-master.comic-website.svc.cluster.local"' outputs.json)
          REDIS_PORT=$(jq -r '.redis_port.value // "6379"' outputs.json)
          
          # ä½¿ç”¨ç¡¬ç¼–ç çš„ Community æ•°æ®åº“ä¿¡æ¯
          COMMUNITY_DB_NAME="communitydb"
          COMMUNITY_DB_USERNAME="community_user"
          
          # å¦‚æœ community_ecr_url ä¸ºç©ºï¼Œä½¿ç”¨ backend_ecr_url ä½œä¸ºå¤‡ç”¨
          if [ -z "$COMMUNITY_ECR_URL" ]; then
            echo "âš ï¸ WARNING: COMMUNITY_ECR_URL is empty, using BACKEND_ECR_URL as fallback"
            COMMUNITY_ECR_URL="$BACKEND_ECR_URL"
          fi
          
          echo "backend_ecr_url=$BACKEND_ECR_URL" >> $GITHUB_OUTPUT
          echo "frontend_ecr_url=$FRONTEND_ECR_URL" >> $GITHUB_OUTPUT
          echo "community_ecr_url=$COMMUNITY_ECR_URL" >> $GITHUB_OUTPUT
          echo "rds_endpoint=$RDS_ENDPOINT" >> $GITHUB_OUTPUT
          echo "rds_port=$RDS_PORT" >> $GITHUB_OUTPUT
          echo "rds_username=$RDS_USERNAME" >> $GITHUB_OUTPUT
          echo "rds_database=$RDS_DATABASE" >> $GITHUB_OUTPUT
          echo "s3_bucket_name=$S3_BUCKET_NAME" >> $GITHUB_OUTPUT
          echo "s3_bucket_region=$S3_BUCKET_REGION" >> $GITHUB_OUTPUT
          echo "alb_controller_role_arn=$ALB_CONTROLLER_ROLE_ARN" >> $GITHUB_OUTPUT
          echo "backend_role_arn=$BACKEND_ROLE_ARN" >> $GITHUB_OUTPUT
          echo "frontend_role_arn=$FRONTEND_ROLE_ARN" >> $GITHUB_OUTPUT
          echo "community_role_arn=$COMMUNITY_ROLE_ARN" >> $GITHUB_OUTPUT
          echo "alb_url=$ALB_URL" >> $GITHUB_OUTPUT
          echo "redis_host=$REDIS_HOST" >> $GITHUB_OUTPUT
          echo "redis_port=$REDIS_PORT" >> $GITHUB_OUTPUT
          
          # ä½¿ç”¨ç¡¬ç¼–ç å€¼
          echo "community_database_name=$COMMUNITY_DB_NAME" >> $GITHUB_OUTPUT
          echo "community_database_username=$COMMUNITY_DB_USERNAME" >> $GITHUB_OUTPUT

      - name: â˜¸ï¸ Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

  build-images:
    name: "Build Docker Images"
    runs-on: ubuntu-latest
    needs: terraform-infrastructure
    environment: production
    if: needs.terraform-infrastructure.result == 'success'
    
    outputs:
      backend_image: ${{ steps.build-backend.outputs.image_tag }}
      frontend_image: ${{ steps.build-frontend.outputs.image_tag }}
      community_image: ${{ steps.build-community.outputs.image_tag }}

    steps:
      - name: ğŸ§© Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”‘ Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: ğŸ” Get AWS Account ID and ECR Registry
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "ACCOUNT_ID=$ACCOUNT_ID" >> $GITHUB_ENV
          ECR_REGISTRY="$ACCOUNT_ID.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com"
          echo "ECR_REGISTRY=$ECR_REGISTRY" >> $GITHUB_ENV

      - name: ğŸ” Login to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin $ECR_REGISTRY

      - name: ğŸ” Set ECR URLs
        run: |
          BACKEND_ECR_URL="${{ needs.terraform-infrastructure.outputs.backend_ecr_url }}"
          FRONTEND_ECR_URL="${{ needs.terraform-infrastructure.outputs.frontend_ecr_url }}"
          COMMUNITY_ECR_URL="${{ needs.terraform-infrastructure.outputs.community_ecr_url }}"
          
          # å¦‚æœ community_ecr_url ä¸ºç©ºï¼Œä½¿ç”¨ backend_ecr_url
          if [ -z "$COMMUNITY_ECR_URL" ]; then
            echo "âš ï¸ WARNING: COMMUNITY_ECR_URL is empty, using BACKEND_ECR_URL as fallback"
            COMMUNITY_ECR_URL="$BACKEND_ECR_URL"
          fi
          
          echo "BACKEND_ECR_URL=$BACKEND_ECR_URL" >> $GITHUB_ENV
          echo "FRONTEND_ECR_URL=$FRONTEND_ECR_URL" >> $GITHUB_ENV
          echo "COMMUNITY_ECR_URL=$COMMUNITY_ECR_URL" >> $GITHUB_ENV

      - name: ğŸ— Check if Backend Image Exists
        id: check-backend
        run: |
          IMAGE_TAG="${GITHUB_SHA:0:8}"
          BACKEND_IMAGE="$BACKEND_ECR_URL:$IMAGE_TAG"
          if docker manifest inspect $BACKEND_IMAGE > /dev/null 2>&1; then
            echo "image_exists=true" >> $GITHUB_OUTPUT
            echo "image_tag=$BACKEND_IMAGE" >> $GITHUB_OUTPUT
          else
            echo "image_exists=false" >> $GITHUB_OUTPUT
            echo "image_tag=$BACKEND_IMAGE" >> $GITHUB_OUTPUT
          fi

      - name: ğŸ§± Build Backend Image (if needed)
        id: build-backend
        if: steps.check-backend.outputs.image_exists == 'false'
        run: |
          cd backend
          IMAGE_TAG="${GITHUB_SHA:0:8}"
          BACKEND_IMAGE="$BACKEND_ECR_URL:$IMAGE_TAG"
          echo "Building and pushing backend image: $BACKEND_IMAGE"
          
          # ä½¿ç”¨ --no-cache ç¡®ä¿é‡æ–°æ„å»º
          docker build --no-cache -t $BACKEND_IMAGE .
          docker push $BACKEND_IMAGE
          echo "image_tag=$BACKEND_IMAGE" >> $GITHUB_OUTPUT

      - name: ğŸ¨ Build Frontend Image
        id: build-frontend
        run: |
          cd frontend
          IMAGE_TAG="${GITHUB_SHA:0:8}"
          FRONTEND_IMAGE="$FRONTEND_ECR_URL:$IMAGE_TAG"
          echo "Building and pushing frontend image: $FRONTEND_IMAGE"
          docker build -t $FRONTEND_IMAGE .
          docker push $FRONTEND_IMAGE
          echo "image_tag=$FRONTEND_IMAGE" >> $GITHUB_OUTPUT

      - name: ğŸ’¬ Build Community Service Image
        id: build-community
        run: |
          cd community-service
          IMAGE_TAG="${GITHUB_SHA:0:8}"
          COMMUNITY_IMAGE="$COMMUNITY_ECR_URL:$IMAGE_TAG"
          echo "Building and pushing community service image: $COMMUNITY_IMAGE"
          docker build -t $COMMUNITY_IMAGE .
          docker push $COMMUNITY_IMAGE
          echo "image_tag=$COMMUNITY_IMAGE" >> $GITHUB_OUTPUT

  configure-rds-database:
    name: "Configure RDS Database"
    runs-on: ubuntu-latest
    needs: 
      - terraform-infrastructure
      - build-images
    environment: production
    if: needs.terraform-infrastructure.result == 'success'

    steps:
      - name: ğŸ§© Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”‘ Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: â˜¸ï¸ Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: ğŸ§° Setup kubectl
        run: |
          sudo snap install kubectl --classic

      - name: ğŸ—ƒ Create Namespace
        run: |
          kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -

      - name: ğŸ§¹ Clean Up Existing Database Jobs
        run: |
          kubectl delete job db-migration -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete job db-migration-debug -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete job community-db-setup -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete job community-schema-setup -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete job community-db-setup-debug -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete job community-schema-setup-debug -n ${{ env.NAMESPACE }} --ignore-not-found=true
          sleep 5

      - name: ğŸ”§ Update Kubernetes Secrets for Database
        run: |
          export RDS_ENDPOINT="${{ needs.terraform-infrastructure.outputs.rds_endpoint }}"
          export RDS_PORT="${{ needs.terraform-infrastructure.outputs.rds_port }}"
          export RDS_USERNAME="${{ needs.terraform-infrastructure.outputs.rds_username }}"
          export RDS_DATABASE="${{ needs.terraform-infrastructure.outputs.rds_database }}"
          export JWT_SECRET="${{ secrets.JWT_SECRET }}"
          export RDS_HOST=$(echo "$RDS_ENDPOINT" | cut -d':' -f1)
          export RDS_PASSWORD="${{ secrets.DB_PASSWORD }}"
          
          # æ–°å¢ Community æ•°æ®åº“ç¯å¢ƒå˜é‡
          export COMMUNITY_DB_NAME="communitydb"
          export COMMUNITY_DB_USERNAME="community_user"
          export COMMUNITY_DB_PASSWORD="${{ secrets.COMMUNITY_DB_PASSWORD }}"
          export COMMUNITY_JWT_SECRET="${{ secrets.COMMUNITY_JWT_SECRET }}"
          
          envsubst < k8s/configs/rds-secret.yaml > k8s/configs/rds-secret.yaml.tmp
          mv k8s/configs/rds-secret.yaml.tmp k8s/configs/rds-secret.yaml
          
          envsubst < k8s/configs/backend-secret.yaml > k8s/configs/backend-secret.yaml.tmp
          mv k8s/configs/backend-secret.yaml.tmp k8s/configs/backend-secret.yaml

      - name: ğŸš€ Apply Database Secrets
        run: |
          cd k8s
          kubectl apply -f configs/rds-secret.yaml
          kubectl apply -f configs/backend-secret.yaml

      - name: ğŸ”§ Create Debug Migration Job for Scripts Check
        run: |
          cat > k8s/migrations/db-migration-debug.yaml << EOF
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: db-migration-debug
            namespace: ${{ env.NAMESPACE }}
          spec:
            template:
              spec:
                containers:
                - name: migration
                  image: ${{ needs.build-images.outputs.backend_image }}
                  command: ["/bin/sh"]
                  args: 
                    - "-c"
                    - |
                      echo "=== å¼€å§‹è°ƒè¯•ï¼šæ£€æŸ¥å®¹å™¨å†…æ–‡ä»¶ç³»ç»Ÿ ==="
                      echo "å½“å‰å·¥ä½œç›®å½•:"
                      pwd
                      echo ""
                      echo "æ ¹ç›®å½•å†…å®¹:"
                      ls -la /
                      echo ""
                      echo "app ç›®å½•å†…å®¹:"
                      ls -la /app
                      echo ""
                      echo "æ£€æŸ¥ scripts æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨:"
                      find /app -name "scripts" -type d 2>/dev/null || echo "scripts æ–‡ä»¶å¤¹æœªæ‰¾åˆ°"
                      echo ""
                      echo "å¦‚æœæ‰¾åˆ° scripts æ–‡ä»¶å¤¹ï¼Œæ˜¾ç¤ºå…¶å†…å®¹:"
                      if [ -d "/app/scripts" ]; then
                        echo "scripts æ–‡ä»¶å¤¹å†…å®¹:"
                        ls -la /app/scripts/
                        echo ""
                        echo "æ£€æŸ¥ migrate.js æ–‡ä»¶:"
                        if [ -f "/app/scripts/migrate.js" ]; then
                          echo "migrate.js æ–‡ä»¶å­˜åœ¨"
                          echo "æ–‡ä»¶å†…å®¹å‰å‡ è¡Œ:"
                          head -20 /app/scripts/migrate.js
                        else
                          echo "migrate.js æ–‡ä»¶ä¸å­˜åœ¨"
                          echo "åœ¨ /app ç›®å½•ä¸­æŸ¥æ‰¾ .js æ–‡ä»¶:"
                          find /app -name "*.js" -type f 2>/dev/null | head -10
                        fi
                      fi
                      echo ""
                      echo "ç¯å¢ƒå˜é‡:"
                      env | grep -E "(DB|DATABASE|RDS)" | sort
                      echo ""
                      echo "=== è°ƒè¯•ä¿¡æ¯è¾“å‡ºå®Œæˆ ==="
                      echo "å®¹å™¨å°†ä¿æŒè¿è¡Œ 1 å°æ—¶ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›å…¥æ£€æŸ¥:"
                      echo "kubectl exec -it \$(kubectl get pods -l job-name=db-migration-debug -o jsonpath='{.items[0].metadata.name}') -n ${{ env.NAMESPACE }} -- /bin/sh"
                      sleep 3600
                  envFrom:
                  - secretRef:
                      name: backend-secret
                  - secretRef:
                      name: rds-secret
                restartPolicy: Never
            backoffLimit: 0
          EOF

      - name: ğŸš€ Run Debug Migration Job
        run: |
          kubectl apply -f k8s/migrations/db-migration-debug.yaml
          echo "ç­‰å¾…è°ƒè¯• Job å¯åŠ¨..."
          sleep 30
          
          # è·å– Pod åç§°
          POD_NAME=$(kubectl get pods -n ${{ env.NAMESPACE }} -l job-name=db-migration-debug -o jsonpath='{.items[0].metadata.name}')
          echo "è°ƒè¯• Pod åç§°: $POD_NAME"
          
          # æ˜¾ç¤º Pod æ—¥å¿—
          echo "=== è°ƒè¯• Pod æ—¥å¿— ==="
          kubectl logs $POD_NAME -n ${{ env.NAMESPACE }}
          echo "=== æ—¥å¿—ç»“æŸ ==="
          
          echo ""
          echo "ğŸ” ç°åœ¨ä½ å¯ä»¥è¿›å…¥ Pod æ£€æŸ¥ scripts æ–‡ä»¶å¤¹:"
          echo "kubectl exec -it $POD_NAME -n ${{ env.NAMESPACE }} -- /bin/sh"
          echo ""
          echo "åœ¨å®¹å™¨å†…å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤:"
          echo "  ls -la /app/scripts/"
          echo "  find /app -name 'migrate.js'"
          echo "  ls -la /"
          echo ""
          echo "âš ï¸  æ³¨æ„: è¿™ä¸ª Pod ä¼šè¿è¡Œ 1 å°æ—¶ï¼Œæ£€æŸ¥å®Œæˆåè¯·æ‰‹åŠ¨åˆ é™¤:"
          echo "kubectl delete job db-migration-debug -n ${{ env.NAMESPACE }}"

      - name: ğŸ—ƒ Create Community Database (Debug Version)
        run: |
          cat > k8s/migrations/community-db-setup-debug.yaml << EOF
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: community-db-setup-debug
            namespace: ${{ env.NAMESPACE }}
          spec:
            template:
              spec:
                containers:
                - name: community-db-setup
                  image: ${{ needs.build-images.outputs.backend_image }}
                  command: ["/bin/sh"]
                  args: 
                    - "-c"
                    - |
                      echo "=== Community æ•°æ®åº“è°ƒè¯• ==="
                      echo "æ£€æŸ¥å®¹å™¨å†…æ–‡ä»¶ç³»ç»Ÿ..."
                      echo "å½“å‰ç›®å½•: \$(pwd)"
                      echo "æ ¹ç›®å½•å†…å®¹:"
                      ls -la /
                      echo ""
                      echo "app ç›®å½•å†…å®¹:"
                      ls -la /app
                      echo ""
                      echo "æŸ¥æ‰¾ SQL æ–‡ä»¶:"
                      find /app -name "*.sql" -type f 2>/dev/null
                      echo ""
                      echo "ç¯å¢ƒå˜é‡:"
                      env | grep -E "(COMMUNITY|DB|DATABASE)" | sort
                      echo ""
                      echo "=== è°ƒè¯•ä¿¡æ¯è¾“å‡ºå®Œæˆ ==="
                      echo "å®¹å™¨å°†ä¿æŒè¿è¡Œ 1 å°æ—¶..."
                      sleep 3600
                  envFrom:
                  - secretRef:
                      name: backend-secret
                  - secretRef:
                      name: rds-secret
                restartPolicy: Never
            backoffLimit: 0
          EOF
          
          kubectl apply -f k8s/migrations/community-db-setup-debug.yaml
          sleep 20
          
          # è·å– Pod åç§°å¹¶æ˜¾ç¤ºæ—¥å¿—
          POD_NAME=$(kubectl get pods -n ${{ env.NAMESPACE }} -l job-name=community-db-setup-debug -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "not-found")
          if [ "$POD_NAME" != "not-found" ]; then
            echo "=== Community DB Setup è°ƒè¯•æ—¥å¿— ==="
            kubectl logs $POD_NAME -n ${{ env.NAMESPACE }}
            echo "=== æ—¥å¿—ç»“æŸ ==="
          fi

      - name: ğŸ“ Manual Intervention Required
        run: |
          echo "================================================"
          echo "ğŸ”§ æ‰‹åŠ¨è°ƒè¯•æ¨¡å¼å·²å¯ç”¨"
          echo "================================================"
          echo ""
          echo "ç°åœ¨ä½ éœ€è¦æ‰‹åŠ¨è¿›å…¥ Pod æ£€æŸ¥ scripts æ–‡ä»¶å¤¹:"
          echo ""
          echo "1. æ£€æŸ¥ä¸»æ•°æ®åº“è¿ç§»è„šæœ¬:"
          echo "   kubectl exec -it \$(kubectl get pods -l job-name=db-migration-debug -o jsonpath='{.items[0].metadata.name}') -n ${{ env.NAMESPACE }} -- /bin/sh"
          echo ""
          echo "2. åœ¨å®¹å™¨å†…è¿è¡Œä»¥ä¸‹å‘½ä»¤:"
          echo "   ls -la /app/"
          echo "   find /app -name scripts"
          echo "   ls -la /app/scripts/  # å¦‚æœå­˜åœ¨"
          echo "   cat /app/scripts/migrate.js  # å¦‚æœå­˜åœ¨"
          echo ""
          echo "3. æ£€æŸ¥ Community æ•°æ®åº“è®¾ç½®:"
          echo "   kubectl exec -it \$(kubectl get pods -l job-name=community-db-setup-debug -o jsonpath='{.items[0].metadata.name}') -n ${{ env.NAMESPACE }} -- /bin/sh"
          echo ""
          echo "4. è°ƒè¯•å®Œæˆåï¼Œç»§ç»­å·¥ä½œæµæ‰§è¡Œ:"
          echo "   - åœ¨ GitHub Actions ç•Œé¢ç‚¹å‡» 'Skip workflow' è·³è¿‡å‰©ä½™æ­¥éª¤"
          echo "   - æˆ–è€…ç­‰å¾… 1 å°æ—¶åè¶…æ—¶ç»§ç»­"
          echo ""
          echo "5. ä¿®å¤é—®é¢˜åï¼Œé‡æ–°è¿è¡Œå·¥ä½œæµ"
          echo "================================================"

      - name: â° Wait for Manual Debugging (Optional)
        run: |
          echo "ç­‰å¾…æ‰‹åŠ¨è°ƒè¯•... (15åˆ†é’Ÿè¶…æ—¶)"
          sleep 900
          echo "ç»§ç»­æ‰§è¡Œå·¥ä½œæµ..."

      - name: ğŸ§¹ Cleanup Debug Jobs
        run: |
          kubectl delete job db-migration-debug -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete job community-db-setup-debug -n ${{ env.NAMESPACE }} --ignore-not-found=true

      - name: âœ… Verify Database Configuration
        run: |
          echo "æ•°æ®åº“è°ƒè¯•å®Œæˆï¼Œè¯·æ ¹æ®æ£€æŸ¥ç»“æœä¿®å¤ Dockerfile æˆ–æ„å»ºè¿‡ç¨‹"

  deploy-frontend:
    name: "Deploy Frontend Service to EKS"
    runs-on: ubuntu-latest
    needs: 
      - terraform-infrastructure
      - build-images
      - configure-rds-database
    environment: production
    if: needs.terraform-infrastructure.result == 'success'

    steps:
      - name: ğŸ§© Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”‘ Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: â˜¸ï¸ Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: ğŸ§° Setup kubectl
        run: |
          sudo snap install kubectl --classic

      - name: ğŸ”§ Install envsubst
        run: |
          sudo apt-get update
          sudo apt-get install -y gettext-base

      - name: ğŸ”§ Update Service Account Role ARNs
        run: |
          ALB_ROLE_ARN="${{ needs.terraform-infrastructure.outputs.alb_controller_role_arn }}"
          FRONTEND_ROLE_ARN="${{ needs.terraform-infrastructure.outputs.frontend_role_arn }}"
          
          sed -i "s|ALB_ROLE_ARN_PLACEHOLDER|$ALB_ROLE_ARN|g" k8s/service-accounts/alb-service-account.yaml
          sed -i "s|FRONTEND_ROLE_ARN_PLACEHOLDER|$FRONTEND_ROLE_ARN|g" k8s/service-accounts/frontend-service-account.yaml

      - name: ğŸ”§ Apply ALB Controller ServiceAccount
        run: |
          kubectl delete serviceaccount aws-load-balancer-controller -n kube-system --ignore-not-found=true
          kubectl apply -f k8s/service-accounts/alb-service-account.yaml

      - name: ğŸ”„ Restart ALB Controller
        run: |
          kubectl rollout restart deployment/aws-load-balancer-controller -n kube-system
          kubectl rollout status deployment/aws-load-balancer-controller -n kube-system --timeout=300s

      - name: ğŸ”§ Update Frontend Secrets
        run: |
          export S3_BUCKET_NAME="${{ needs.terraform-infrastructure.outputs.s3_bucket_name }}"
          export AWS_REGION="${{ env.AWS_REGION }}"
          
          envsubst < k8s/configs/s3-secret.yaml > k8s/configs/s3-secret.yaml.tmp
          mv k8s/configs/s3-secret.yaml.tmp k8s/configs/s3-secret.yaml

      - name: ğŸš€ Deploy Frontend Resources
        run: |
          cd k8s
          kubectl apply -f namespaces/comic-website.yaml
          kubectl apply -f service-accounts/alb-service-account.yaml
          kubectl apply -f service-accounts/frontend-service-account.yaml
          kubectl apply -f configs/frontend-config.yaml
          kubectl apply -f configs/s3-secret.yaml
          kubectl apply -f service/frontend-service.yaml
          kubectl apply -f networking/alb-ingress-class.yaml
          kubectl apply -f networking/alb-ingress.yaml

      - name: ğŸš€ Deploy Frontend Application
        run: |
          cd k8s
          if kubectl get deployment frontend-deployment -n ${{ env.NAMESPACE }} &> /dev/null; then
            kubectl replace -f deployments/frontend-deployment.yaml -n ${{ env.NAMESPACE }} --force
          else
            kubectl apply -f deployments/frontend-deployment.yaml -n ${{ env.NAMESPACE }}
          fi

      - name: ğŸ”„ Update Frontend Image
        run: |
          FRONTEND_IMAGE="${{ needs.build-images.outputs.frontend_image }}"
          
          kubectl patch deployment frontend-deployment -n ${{ env.NAMESPACE }} \
            -p='{"spec":{"template":{"spec":{"containers":[{"name":"frontend","image":"'"$FRONTEND_IMAGE"'"}]}}}}'

      - name: â³ Wait for Frontend Deployment
        run: |
          kubectl rollout status deployment/frontend-deployment -n ${{ env.NAMESPACE }} --timeout=180s

      - name: âœ… Verify Frontend Service
        run: |
          echo "Frontend service deployed successfully"

  deploy-backend:
    name: "Deploy Backend Service to EKS"
    runs-on: ubuntu-latest
    needs: 
      - terraform-infrastructure
      - build-images
      - configure-rds-database
      - deploy-frontend
    environment: production
    if: needs.terraform-infrastructure.result == 'success'

    steps:
      - name: ğŸ§© Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”‘ Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: â˜¸ï¸ Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: ğŸ§° Setup kubectl
        run: |
          sudo snap install kubectl --classic

      - name: ğŸ”§ Update Backend Service Account Role ARN
        run: |
          BACKEND_ROLE_ARN="${{ needs.terraform-infrastructure.outputs.backend_role_arn }}"
          sed -i "s|BACKEND_ROLE_ARN_PLACEHOLDER|$BACKEND_ROLE_ARN|g" k8s/service-accounts/backend-service-account.yaml

      - name: ğŸš€ Deploy Backend Resources
        run: |
          cd k8s
          kubectl apply -f service-accounts/backend-service-account.yaml
          kubectl apply -f configs/backend-config.yaml
          kubectl apply -f service/backend-service.yaml

      - name: ğŸš€ Deploy Backend Application
        run: |
          cd k8s
          if kubectl get deployment backend-deployment -n ${{ env.NAMESPACE }} &> /dev/null; then
            kubectl replace -f deployments/backend-deployment.yaml -n ${{ env.NAMESPACE }} --force
          else
            kubectl apply -f deployments/backend-deployment.yaml -n ${{ env.NAMESPACE }}
          fi

      - name: ğŸ”„ Update Backend Image
        run: |
          BACKEND_IMAGE="${{ needs.build-images.outputs.backend_image }}"
          
          kubectl patch deployment backend-deployment -n ${{ env.NAMESPACE }} \
            -p='{"spec":{"template":{"spec":{"containers":[{"name":"backend","image":"'"$BACKEND_IMAGE"'"}]}}}}'

      - name: â³ Wait for Backend Deployment
        run: |
          kubectl rollout status deployment/backend-deployment -n ${{ env.NAMESPACE }} --timeout=300s

      - name: âœ… Verify Backend Service
        run: |
          echo "Backend service deployed successfully"

  deploy-community:
    name: "Deploy Community Service to EKS"
    runs-on: ubuntu-latest
    needs: 
      - terraform-infrastructure
      - build-images
      - configure-rds-database
      - deploy-backend
    environment: production
    if: needs.terraform-infrastructure.result == 'success'

    steps:
      - name: ğŸ§© Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”‘ Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: â˜¸ï¸ Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: ğŸ§° Setup kubectl
        run: |
          sudo snap install kubectl --classic

      - name: ğŸ”§ Install envsubst
        run: |
          sudo apt-get update
          sudo apt-get install -y gettext-base

      - name: ğŸ”§ Update Community Service Account Role ARN
        run: |
          COMMUNITY_ROLE_ARN="${{ needs.terraform-infrastructure.outputs.community_role_arn }}"
          sed -i "s|COMMUNITY_ROLE_ARN_PLACEHOLDER|$COMMUNITY_ROLE_ARN|g" k8s/service-accounts/community-service-account.yaml

      - name: ğŸ”§ Update Community Secrets
        run: |
          export RDS_ENDPOINT="${{ needs.terraform-infrastructure.outputs.rds_endpoint }}"
          export RDS_PORT="${{ needs.terraform-infrastructure.outputs.rds_port }}"
          export RDS_USERNAME="${{ needs.terraform-infrastructure.outputs.rds_username }}"
          export RDS_DATABASE="${{ needs.terraform-infrastructure.outputs.rds_database }}"
          export RDS_HOST=$(echo "$RDS_ENDPOINT" | cut -d':' -f1)
          export RDS_PASSWORD="${{ secrets.DB_PASSWORD }}"
          
          # æ–°å¢ Community æ•°æ®åº“ç¯å¢ƒå˜é‡
          export COMMUNITY_DB_NAME="communitydb"
          export COMMUNITY_DB_USERNAME="community_user"
          export COMMUNITY_DB_PASSWORD="${{ secrets.COMMUNITY_DB_PASSWORD }}"
          export COMMUNITY_JWT_SECRET="${{ secrets.COMMUNITY_JWT_SECRET }}"
          
          # ä» Terraform è¾“å‡ºè·å– Redis é…ç½®
          export REDIS_HOST="${{ needs.terraform-infrastructure.outputs.redis_host }}"
          export REDIS_PORT="${{ needs.terraform-infrastructure.outputs.redis_port }}"
          export REDIS_PASSWORD="${{ secrets.REDIS_PASSWORD }}"
          
          envsubst < k8s/configs/community-secret.yaml > k8s/configs/community-secret.yaml.tmp
          mv k8s/configs/community-secret.yaml.tmp k8s/configs/community-secret.yaml

      - name: ğŸš€ Deploy Community Resources
        run: |
          cd k8s
          kubectl apply -f service-accounts/community-service-account.yaml
          kubectl apply -f configs/community-configmap.yaml
          kubectl apply -f configs/community-secret.yaml
          kubectl apply -f service/community-service.yaml

      - name: ğŸš€ Deploy Community Application
        run: |
          cd k8s
          if kubectl get deployment community-deployment -n ${{ env.NAMESPACE }} &> /dev/null; then
            kubectl replace -f deployments/community-deployment.yaml -n ${{ env.NAMESPACE }} --force
          else
            kubectl apply -f deployments/community-deployment.yaml -n ${{ env.NAMESPACE }}
          fi

      - name: ğŸ”„ Update Community Image
        run: |
          COMMUNITY_IMAGE="${{ needs.build-images.outputs.community_image }}"
          
          kubectl patch deployment community-deployment -n ${{ env.NAMESPACE }} \
            -p='{"spec":{"template":{"spec":{"containers":[{"name":"community-service","image":"'"$COMMUNITY_IMAGE"'"}]}}}}'

      - name: â³ Wait for Community Deployment
        run: |
          kubectl rollout status deployment/community-deployment -n ${{ env.NAMESPACE }} --timeout=180s

      - name: âœ… Verify Community Service
        run: |
          echo "Community service deployed successfully"

      - name: ğŸŒ Get ALB URL
        id: get-alb-url
        run: |
          for i in {1..15}; do
            ALB_HOSTNAME=$(kubectl get ingress comic-website-ingress -n ${{ env.NAMESPACE }} -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            if [ -n "$ALB_HOSTNAME" ]; then
              echo "alb_url=http://$ALB_HOSTNAME" >> $GITHUB_OUTPUT
              break
            else
              sleep 20
            fi
          done

      - name: ğŸ“¢ Display Application URL
        run: |
          echo "================================================"
          echo "ğŸš€ All Services Deployed Successfully!"
          echo "================================================"
          echo ""
          echo "ğŸŒ Access URL:"
          echo "   ${{ steps.get-alb-url.outputs.alb_url }}"
          echo ""
          echo "ğŸ“Š Services Status:"
          echo "   âœ… Frontend Service: Ready"
          echo "   âœ… Backend API Service: Ready" 
          echo "   âœ… Community Service: Ready"
          echo "   âœ… Database Configuration: Complete"
          echo ""
          echo "âš ï¸  IMPORTANT: Database migrations were skipped for debugging"
          echo "   Run migrations manually after fixing scripts issue:"
          echo "   kubectl run db-migration --image=${{ needs.build-images.outputs.backend_image }} --restart=Never --rm -it -- node scripts/migrate.js"
          echo "================================================"