name: Deploy Comic Website

on:
  push:
    branches: [main]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: comic-website-prod
  PROJECT_NAME: comic-website
  ENVIRONMENT: prod
  NAMESPACE: comic-website
  TF_BACKEND_BUCKET: comic-website-tfstate-2024

jobs:
  terraform-infrastructure:
    name: "Terraform - Create Infrastructure"
    runs-on: ubuntu-latest
    environment: production
    outputs:
      backend_ecr_url: ${{ steps.terraform-outputs.outputs.backend_ecr_url }}
      frontend_ecr_url: ${{ steps.terraform-outputs.outputs.frontend_ecr_url }}
      community_ecr_url: ${{ steps.terraform-outputs.outputs.community_ecr_url }}
      rds_endpoint: ${{ steps.terraform-outputs.outputs.rds_endpoint }}
      rds_port: ${{ steps.terraform-outputs.outputs.rds_port }}
      rds_username: ${{ steps.terraform-outputs.outputs.rds_username }}
      rds_database: ${{ steps.terraform-outputs.outputs.rds_database }}
      s3_bucket_name: ${{ steps.terraform-outputs.outputs.s3_bucket_name }}
      s3_bucket_region: ${{ steps.terraform-outputs.outputs.s3_bucket_region }}
      alb_controller_role_arn: ${{ steps.terraform-outputs.outputs.alb_controller_role_arn }}
      backend_role_arn: ${{ steps.terraform-outputs.outputs.backend_role_arn }}
      frontend_role_arn: ${{ steps.terraform-outputs.outputs.frontend_role_arn }}
      community_role_arn: ${{ steps.terraform-outputs.outputs.community_role_arn }}
      alb_url: ${{ steps.terraform-outputs.outputs.alb_url }}
      redis_host: ${{ steps.terraform-outputs.outputs.redis_host }}
      redis_port: ${{ steps.terraform-outputs.outputs.redis_port }}
      community_database_name: ${{ steps.terraform-outputs.outputs.community_database_name }}
      community_database_username: ${{ steps.terraform-outputs.outputs.community_database_username }}

    steps:
      - name: 🧩 Checkout code
        uses: actions/checkout@v4

      - name: 🔑 Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: 🏗 Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.0

      - name: ⚙️ Terraform Init
        run: |
          cd terraform
          terraform init \
            -backend-config="bucket=${{ env.TF_BACKEND_BUCKET }}" \
            -backend-config="key=terraform/state/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="encrypt=true" \
            -backend-config="dynamodb_table=comic-website-tfstate-lock"

      - name: ✅ Terraform Validate
        run: |
          cd terraform
          terraform validate

      - name: 🚀 Terraform Plan and Apply
        run: |
          cd terraform
          terraform plan -out=tfplan \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="project_name=${{ env.PROJECT_NAME }}" \
            -var="environment=${{ env.ENVIRONMENT }}" \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="community_db_password=${{ secrets.COMMUNITY_DB_PASSWORD }}" \
            -var="community_db_username=${{ secrets.COMMUNITY_DB_USERNAME }}" \
            -var="community_db_name=${{ secrets.COMMUNITY_DB_NAME }}"
          terraform apply -auto-approve tfplan

      - name: 📋 Get Terraform Outputs
        id: terraform-outputs
        run: |
          cd terraform
          terraform output -json > outputs.json
          
          BACKEND_ECR_URL=$(jq -r '.backend_repository_url.value // empty' outputs.json)
          FRONTEND_ECR_URL=$(jq -r '.frontend_repository_url.value // empty' outputs.json)
          COMMUNITY_ECR_URL=$(jq -r '.community_repository_url.value // empty' outputs.json)
          RDS_ENDPOINT=$(jq -r '.rds_endpoint.value // empty' outputs.json)
          RDS_PORT=$(jq -r '.rds_port.value // "5432"' outputs.json)
          RDS_USERNAME=$(jq -r '.rds_username.value // "comicadmin"' outputs.json)
          RDS_DATABASE=$(jq -r '.rds_database_name.value // "comicdb"' outputs.json)
          S3_BUCKET_NAME=$(jq -r '.s3_bucket_name.value // empty' outputs.json)
          S3_BUCKET_REGION=$(jq -r '.s3_bucket_region.value // empty' outputs.json)
          ALB_CONTROLLER_ROLE_ARN=$(jq -r '.alb_controller_role_arn.value // empty' outputs.json)
          BACKEND_ROLE_ARN=$(jq -r '.backend_role_arn.value // empty' outputs.json)
          FRONTEND_ROLE_ARN=$(jq -r '.frontend_role_arn.value // empty' outputs.json)
          COMMUNITY_ROLE_ARN=$(jq -r '.community_role_arn.value // empty' outputs.json)
          ALB_URL=$(jq -r '.alb_url.value // empty' outputs.json)
          REDIS_HOST=$(jq -r '.redis_host.value // "redis-master.comic-website.svc.cluster.local"' outputs.json)
          REDIS_PORT=$(jq -r '.redis_port.value // "6379"' outputs.json)
          COMMUNITY_DB_NAME="communitydb"
          COMMUNITY_DB_USERNAME="community_user"
          
          if [ -z "$COMMUNITY_ECR_URL" ]; then
            echo "⚠️ WARNING: COMMUNITY_ECR_URL is empty, creating temporary ECR URL"
            ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            COMMUNITY_ECR_URL="$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/comic-website-prod-community"
          fi
          
          echo "backend_ecr_url=$BACKEND_ECR_URL" >> $GITHUB_OUTPUT
          echo "frontend_ecr_url=$FRONTEND_ECR_URL" >> $GITHUB_OUTPUT
          echo "community_ecr_url=$COMMUNITY_ECR_URL" >> $GITHUB_OUTPUT
          echo "rds_endpoint=$RDS_ENDPOINT" >> $GITHUB_OUTPUT
          echo "rds_port=$RDS_PORT" >> $GITHUB_OUTPUT
          echo "rds_username=$RDS_USERNAME" >> $GITHUB_OUTPUT
          echo "rds_database=$RDS_DATABASE" >> $GITHUB_OUTPUT
          echo "s3_bucket_name=$S3_BUCKET_NAME" >> $GITHUB_OUTPUT
          echo "s3_bucket_region=$S3_BUCKET_REGION" >> $GITHUB_OUTPUT
          echo "alb_controller_role_arn=$ALB_CONTROLLER_ROLE_ARN" >> $GITHUB_OUTPUT
          echo "backend_role_arn=$BACKEND_ROLE_ARN" >> $GITHUB_OUTPUT
          echo "frontend_role_arn=$FRONTEND_ROLE_ARN" >> $GITHUB_OUTPUT
          echo "community_role_arn=$COMMUNITY_ROLE_ARN" >> $GITHUB_OUTPUT
          echo "alb_url=$ALB_URL" >> $GITHUB_OUTPUT
          echo "redis_host=$REDIS_HOST" >> $GITHUB_OUTPUT
          echo "redis_port=$REDIS_PORT" >> $GITHUB_OUTPUT
          echo "community_database_name=$COMMUNITY_DB_NAME" >> $GITHUB_OUTPUT
          echo "community_database_username=$COMMUNITY_DB_USERNAME" >> $GITHUB_OUTPUT

      - name: ☸️ Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

  build-images:
    name: "Build Docker Images"
    runs-on: ubuntu-latest
    needs: terraform-infrastructure
    environment: production
    if: needs.terraform-infrastructure.result == 'success'
    
    outputs:
      backend_image: ${{ steps.build-backend.outputs.image_tag }}
      frontend_image: ${{ steps.build-frontend.outputs.image_tag }}
      community_image: ${{ steps.build-community.outputs.image_tag }}
      backend_image_tag: ${{ steps.build-backend.outputs.image_tag_only }}
      frontend_image_tag: ${{ steps.build-frontend.outputs.image_tag_only }}
      community_image_tag: ${{ steps.build-community.outputs.image_tag_only }}

    steps:
      - name: 🧩 Checkout code
        uses: actions/checkout@v4

      - name: 🔑 Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: 🔍 Get AWS Account ID and ECR Registry
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "ACCOUNT_ID=$ACCOUNT_ID" >> $GITHUB_ENV
          ECR_REGISTRY="$ACCOUNT_ID.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com"
          echo "ECR_REGISTRY=$ECR_REGISTRY" >> $GITHUB_ENV

      - name: 🔐 Login to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin $ECR_REGISTRY

      - name: 🔍 Set ECR URLs
        run: |
          BACKEND_ECR_URL="${{ needs.terraform-infrastructure.outputs.backend_ecr_url }}"
          FRONTEND_ECR_URL="${{ needs.terraform-infrastructure.outputs.frontend_ecr_url }}"
          COMMUNITY_ECR_URL="${{ needs.terraform-infrastructure.outputs.community_ecr_url }}"
          
          if [ -z "$BACKEND_ECR_URL" ]; then
            echo "❌ ERROR: BACKEND_ECR_URL is required but empty"
            exit 1
          fi
          if [ -z "$FRONTEND_ECR_URL" ]; then
            echo "❌ ERROR: FRONTEND_ECR_URL is required but empty"
            exit 1
          fi
          if [ -z "$COMMUNITY_ECR_URL" ]; then
            echo "❌ ERROR: COMMUNITY_ECR_URL is required but empty"
            exit 1
          fi
          
          echo "BACKEND_ECR_URL=$BACKEND_ECR_URL" >> $GITHUB_ENV
          echo "FRONTEND_ECR_URL=$FRONTEND_ECR_URL" >> $GITHUB_ENV
          echo "COMMUNITY_ECR_URL=$COMMUNITY_ECR_URL" >> $GITHUB_ENV

      - name: 🏗 Check if Backend Image Exists
        id: check-backend
        run: |
          IMAGE_TAG="${GITHUB_SHA:0:8}"
          BACKEND_IMAGE="$BACKEND_ECR_URL:$IMAGE_TAG"
          if docker manifest inspect $BACKEND_IMAGE > /dev/null 2>&1; then
            echo "image_exists=true" >> $GITHUB_OUTPUT
            echo "image_tag=$BACKEND_IMAGE" >> $GITHUB_OUTPUT
            echo "image_tag_only=$IMAGE_TAG" >> $GITHUB_OUTPUT
          else
            echo "image_exists=false" >> $GITHUB_OUTPUT
            echo "image_tag=$BACKEND_IMAGE" >> $GITHUB_OUTPUT
            echo "image_tag_only=$IMAGE_TAG" >> $GITHUB_OUTPUT
          fi

      - name: 🧱 Build Backend Image (if needed)
        id: build-backend
        if: steps.check-backend.outputs.image_exists == 'false'
        run: |
          cd backend
          IMAGE_TAG="${GITHUB_SHA:0:8}"
          BACKEND_IMAGE="$BACKEND_ECR_URL:$IMAGE_TAG"
          echo "Building and pushing backend image: $BACKEND_IMAGE"
          
          echo "=== Backend 构建调试 ==="
          echo "当前目录: $(pwd)"
          echo "目录内容:"
          ls -la
          echo ""
          echo "scripts 文件夹内容:"
          ls -la scripts/ || echo "scripts 文件夹不存在"
          echo ""
          echo "scripts 文件列表:"
          find scripts/ -type f 2>/dev/null || echo "没有找到 scripts 文件"
          echo ""
          echo "Dockerfile 内容:"
          cat Dockerfile
          
          docker build --no-cache -t $BACKEND_IMAGE .
          
          echo "=== Backend 镜像验证 ==="
          echo "1. 检查镜像中的文件结构:"
          docker run --rm $BACKEND_IMAGE ls -la /app/
          echo ""
          echo "2. 检查 scripts 文件夹:"
          docker run --rm $BACKEND_IMAGE ls -la /app/scripts/ || echo "scripts 文件夹不存在"
          echo ""
          echo "3. 查找 migrate.js 文件:"
          docker run --rm $BACKEND_IMAGE find /app -name "migrate.js" -type f 2>/dev/null || echo "migrate.js 文件不存在"
          echo ""
          echo "4. 查找 community-migration.sql 文件:"
          docker run --rm $BACKEND_IMAGE find /app -name "community-migration.sql" -type f 2>/dev/null || echo "community-migration.sql 文件不存在"
          echo ""
          echo "5. 尝试运行 migrate.js:"
          docker run --rm $BACKEND_IMAGE node -e "console.log('Node.js working'); try { require('fs').accessSync('/app/scripts/migrate.js'); console.log('migrate.js exists') } catch(e) { console.log('migrate.js missing:', e.message) }" || echo "无法运行 Node.js 检查"
          
          docker push $BACKEND_IMAGE
          echo "image_tag=$BACKEND_IMAGE" >> $GITHUB_OUTPUT
          echo "image_tag_only=$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: 🎨 Build Frontend Image
        id: build-frontend
        run: |
          cd frontend
          IMAGE_TAG="${GITHUB_SHA:0:8}"
          FRONTEND_IMAGE="$FRONTEND_ECR_URL:$IMAGE_TAG"
          echo "Building and pushing frontend image: $FRONTEND_IMAGE"
          docker build -t $FRONTEND_IMAGE .
          docker push $FRONTEND_IMAGE
          echo "image_tag=$FRONTEND_IMAGE" >> $GITHUB_OUTPUT
          echo "image_tag_only=$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: 💬 Build Community Service Image
        id: build-community
        run: |
          cd community-service
          IMAGE_TAG="${GITHUB_SHA:0:8}"
          COMMUNITY_IMAGE="$COMMUNITY_ECR_URL:$IMAGE_TAG"
          echo "Building and pushing community service image: $COMMUNITY_IMAGE"
          docker build -t $COMMUNITY_IMAGE .
          docker push $COMMUNITY_IMAGE
          echo "image_tag=$COMMUNITY_IMAGE" >> $GITHUB_OUTPUT
          echo "image_tag_only=$IMAGE_TAG" >> $GITHUB_OUTPUT

  configure-rds-database:
    name: "Configure RDS Database"
    runs-on: ubuntu-latest
    needs: 
      - terraform-infrastructure
      - build-images
    environment: production
    if: needs.terraform-infrastructure.result == 'success'

    steps:
      - name: 🧩 Checkout code
        uses: actions/checkout@v4

      - name: 🔑 Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: ☸️ Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: 🧰 Setup kubectl
        run: |
          sudo snap install kubectl --classic

      - name: 🔍 Debug Migration Issue
        run: |
          echo "=== 检查本地文件结构 ==="
          find . -name "migrate.js" -type f
          echo ""
          echo "=== 检查 backend 目录结构 ==="
          cd backend
          ls -la
          echo ""
          echo "=== 检查 scripts 文件夹 ==="
          ls -la scripts/
          echo ""
          echo "=== 检查 Dockerfile ==="
          cat Dockerfile

      - name: 🗃 Create Namespace
        run: |
          kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -

      - name: 🧹 Clean Up Existing Database Jobs
        run: |
          kubectl delete job db-migration -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete job database-migration -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete job community-db-setup -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete job community-schema-setup -n ${{ env.NAMESPACE }} --ignore-not-found=true
          sleep 5

      - name: 🔧 Update Kubernetes Secrets for Database
        run: |
          export RDS_ENDPOINT="${{ needs.terraform-infrastructure.outputs.rds_endpoint }}"
          export RDS_PORT="${{ needs.terraform-infrastructure.outputs.rds_port }}"
          export RDS_USERNAME="${{ needs.terraform-infrastructure.outputs.rds_username }}"
          export RDS_DATABASE="${{ needs.terraform-infrastructure.outputs.rds_database }}"
          export JWT_SECRET="${{ secrets.JWT_SECRET }}"
          export RDS_HOST=$(echo "$RDS_ENDPOINT" | cut -d':' -f1)
          export RDS_PASSWORD="${{ secrets.DB_PASSWORD }}"
          export COMMUNITY_DB_NAME="communitydb"
          export COMMUNITY_DB_USERNAME="community_user"
          export COMMUNITY_DB_PASSWORD="${{ secrets.COMMUNITY_DB_PASSWORD }}"
          export COMMUNITY_JWT_SECRET="${{ secrets.COMMUNITY_JWT_SECRET }}"
          
          envsubst < k8s/configs/rds-secret.yaml > k8s/configs/rds-secret.yaml.tmp
          mv k8s/configs/rds-secret.yaml.tmp k8s/configs/rds-secret.yaml
          envsubst < k8s/configs/backend-secret.yaml > k8s/configs/backend-secret.yaml.tmp
          mv k8s/configs/backend-secret.yaml.tmp k8s/configs/backend-secret.yaml

      - name: 🚀 Apply Database Secrets
        run: |
          cd k8s
          kubectl apply -f configs/rds-secret.yaml
          kubectl apply -f configs/backend-secret.yaml

      - name: 🗃 Run Main Database Migrations
        run: |
          echo "🚀 Running main database migration..."
          
          # 动态更新 migration job 的镜像标签
          BACKEND_IMAGE_TAG="${{ needs.build-images.outputs.backend_image_tag }}"
          BACKEND_ECR_URL="${{ needs.terraform-infrastructure.outputs.backend_ecr_url }}"
          BACKEND_IMAGE="$BACKEND_ECR_URL:$BACKEND_IMAGE_TAG"
          
          echo "Using backend image for migration: $BACKEND_IMAGE"
          
          # 先删除可能存在的旧 job
          kubectl delete job db-migration -n ${{ env.NAMESPACE }} --ignore-not-found=true
          sleep 5
          
          # 使用 sed 替换镜像占位符 - 更可靠的方法
          echo "=== 原始文件内容 ==="
          cat k8s/migrations/db-migration-job.yaml
          
          # 使用 sed 替换 PLACEHOLDER_BACKEND_IMAGE
          sed "s|PLACEHOLDER_BACKEND_IMAGE|$BACKEND_IMAGE|g" k8s/migrations/db-migration-job.yaml > k8s/migrations/db-migration-job-temp.yaml
          
          # 验证替换结果
          echo "=== 替换后的文件内容 ==="
          cat k8s/migrations/db-migration-job-temp.yaml
          echo ""
          echo "=== 验证镜像字段 ==="
          grep "image:" k8s/migrations/db-migration-job-temp.yaml
          
          # 应用迁移 job
          kubectl apply -f k8s/migrations/db-migration-job-temp.yaml
          
          echo "获取迁移 Pod 信息..."
          kubectl get pods -n ${{ env.NAMESPACE }} -l job-name=db-migration
          
          # 等待 Pod 启动
          echo "等待 Pod 启动..."
          for i in {1..30}; do
            POD_STATUS=$(kubectl get pods -n ${{ env.NAMESPACE }} -l job-name=db-migration -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo "NotFound")
            POD_NAME=$(kubectl get pods -n ${{ env.NAMESPACE }} -l job-name=db-migration -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "not-found")
            
            if [ "$POD_STATUS" = "Running" ] || [ "$POD_STATUS" = "Succeeded" ] || [ "$POD_STATUS" = "Failed" ]; then
              echo "Pod 状态: $POD_STATUS"
              break
            fi
            echo "等待 Pod 启动... ($i/30) - 当前状态: $POD_STATUS"
            sleep 5
          done
          
          if [ "$POD_NAME" != "not-found" ]; then
            echo "迁移 Pod 名称: $POD_NAME"
            echo "=== Pod 状态 ==="
            kubectl describe pod $POD_NAME -n ${{ env.NAMESPACE }}
            echo ""
            echo "=== Pod 日志 ==="
            kubectl logs $POD_NAME -n ${{ env.NAMESPACE }} --tail=50 || echo "无法获取日志，Pod 可能还在启动中"
          else
            echo "❌ 无法找到迁移 Pod"
          fi
          
          echo "Waiting for database migration to complete..."
          if kubectl wait --for=condition=complete job/db-migration -n ${{ env.NAMESPACE }} --timeout=300s; then
            echo "✅ Database migration successful"
            kubectl delete job db-migration -n ${{ env.NAMESPACE }}
            rm -f k8s/migrations/db-migration-job-temp.yaml
          else
            echo "❌ Database migration failed"
            # 再次获取日志
            POD_NAME=$(kubectl get pods -n ${{ env.NAMESPACE }} -l job-name=db-migration -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "not-found")
            if [ "$POD_NAME" != "not-found" ]; then
              echo "=== 最终 Pod 日志 ==="
              kubectl logs $POD_NAME -n ${{ env.NAMESPACE }} --tail=100
            fi
            rm -f k8s/migrations/db-migration-job-temp.yaml
            exit 1
          fi

      - name: 🗃 Create Community Database
        run: |
          echo "🚀 Creating Community database..."
          kubectl apply -f k8s/migrations/community-schema-job.yaml

          echo "Waiting for Community database creation to complete..."
          if kubectl wait --for=condition=complete job/community-db-setup -n ${{ env.NAMESPACE }} --timeout=300s; then
            echo "✅ Community database creation successful"
            kubectl delete job community-db-setup -n ${{ env.NAMESPACE }}
          else
            echo "❌ Community database creation failed"
            kubectl logs job/community-db-setup -n ${{ env.NAMESPACE }} --tail=50
            exit 1
          fi

      - name: 🗃 Create Community Database Tables
        run: |
          echo "🚀 Creating Community database tables..."
          kubectl apply -f k8s/migrations/community-tables-job.yaml
          
          echo "Waiting for Community database table structure creation to complete..."
          if kubectl wait --for=condition=complete job/community-tables-setup -n ${{ env.NAMESPACE }} --timeout=300s; then
            echo "✅ Community database table structure creation successful"
            kubectl delete job community-tables-setup -n ${{ env.NAMESPACE }}
          else
            echo "❌ Community database table structure creation failed"
            kubectl logs job/community-tables-setup -n ${{ env.NAMESPACE }} --tail=50
            exit 1
          fi

      - name: ✅ Verify Database Configuration
        run: |
          echo "✅ Database configuration completed successfully"

  deploy-frontend:
    name: "Deploy Frontend Service to EKS"
    runs-on: ubuntu-latest
    needs: [terraform-infrastructure, build-images, configure-rds-database]
    environment: production
    if: needs.terraform-infrastructure.result == 'success'

    steps:
      - name: 🧩 Checkout code
        uses: actions/checkout@v4
      - name: 🔑 Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      - name: ☸️ Update kubeconfig
        run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
      - name: 🚀 Deploy Frontend
        run: |
          cd k8s
          kubectl apply -f namespaces/comic-website.yaml
          kubectl apply -f service-accounts/frontend-service-account.yaml
          kubectl apply -f configs/frontend-config.yaml
          kubectl apply -f service/frontend-service.yaml
          kubectl apply -f networking/alb-ingress-class.yaml
          kubectl apply -f networking/alb-ingress.yaml
          kubectl apply -f deployments/frontend-deployment.yaml
      - name: 🔄 Update Frontend Image
        run: |
          FRONTEND_IMAGE="${{ needs.build-images.outputs.frontend_image }}"
          kubectl patch deployment frontend-deployment -n ${{ env.NAMESPACE }} -p='{"spec":{"template":{"spec":{"containers":[{"name":"frontend","image":"'"$FRONTEND_IMAGE"'"}]}}}}'
      - name: ⏳ Wait for Frontend Deployment
        run: kubectl rollout status deployment/frontend-deployment -n ${{ env.NAMESPACE }} --timeout=180s

  deploy-backend:
    name: "Deploy Backend Service to EKS"
    runs-on: ubuntu-latest
    needs: [terraform-infrastructure, build-images, configure-rds-database, deploy-frontend]
    environment: production
    if: needs.terraform-infrastructure.result == 'success'

    steps:
      - name: 🧩 Checkout code
        uses: actions/checkout@v4
      - name: 🔑 Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      - name: ☸️ Update kubeconfig
        run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
      
      - name: 🔄 Update Backend Deployment with Latest Image
        run: |
          BACKEND_IMAGE="${{ needs.build-images.outputs.backend_image }}"
          echo "Updating backend deployment with image: $BACKEND_IMAGE"
          
          # 使用 kubectl set image
          kubectl set image deployment/backend-deployment -n ${{ env.NAMESPACE }} backend=$BACKEND_IMAGE
          
          echo "✅ Backend image updated successfully"
      
      - name: 🚀 Deploy Backend Resources
        run: |
          cd k8s
          kubectl apply -f service-accounts/backend-service-account.yaml
          kubectl apply -f configs/backend-config.yaml
          kubectl apply -f service/backend-service.yaml
      
      - name: ⏳ Wait for Backend Deployment
        run: kubectl rollout status deployment/backend-deployment -n ${{ env.NAMESPACE }} --timeout=300s
      
      - name: 🔍 Verify Backend Deployment
        run: |
          echo "=== 检查 Backend Pod 状态 ==="
          kubectl get pods -n ${{ env.NAMESPACE }} -l app=backend
          
          echo "=== 检查当前使用的镜像 ==="
          kubectl get deployment backend-deployment -n ${{ env.NAMESPACE }} -o jsonpath='{.spec.template.spec.containers[0].image}'
          echo ""
          
          echo "=== 检查 Pod 日志 ==="
          BACKEND_POD=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=backend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "not-found")
          if [ "$BACKEND_POD" != "not-found" ]; then
            echo "Backend Pod logs:"
            kubectl logs $BACKEND_POD -n ${{ env.NAMESPACE }} --tail=20
          fi

  deploy-community:
    name: "Deploy Community Service to EKS"
    runs-on: ubuntu-latest
    needs: [terraform-infrastructure, build-images, configure-rds-database, deploy-backend]
    environment: production
    if: needs.terraform-infrastructure.result == 'success'

    steps:
      - name: 🧩 Checkout code
        uses: actions/checkout@v4
      - name: 🔑 Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      - name: ☸️ Update kubeconfig
        run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
      
      - name: 🔄 Update Community Deployment with Latest Image
        run: |
          COMMUNITY_IMAGE="${{ needs.build-images.outputs.community_image }}"
          echo "Updating community deployment with image: $COMMUNITY_IMAGE"
          
          # 使用 kubectl set image 更新社区服务镜像
          kubectl set image deployment/community-deployment -n ${{ env.NAMESPACE }} community-service=$COMMUNITY_IMAGE
          
          echo "✅ Community image updated successfully"
      
      - name: 🚀 Deploy Community Resources
        run: |
          cd k8s
          kubectl apply -f service-accounts/community-service-account.yaml
          kubectl apply -f configs/community-configmap.yaml
          kubectl apply -f configs/community-secret.yaml
          kubectl apply -f service/community-service.yaml
      
      - name: ⏳ Wait for Community Deployment
        run: kubectl rollout status deployment/community-deployment -n ${{ env.NAMESPACE }} --timeout=180s
      
      - name: 🌐 Get ALB URL
        id: get-alb-url
        run: |
          for i in {1..15}; do
            ALB_HOSTNAME=$(kubectl get ingress comic-website-ingress -n ${{ env.NAMESPACE }} -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            if [ -n "$ALB_HOSTNAME" ]; then
              echo "alb_url=http://$ALB_HOSTNAME" >> $GITHUB_OUTPUT
              break
            else
              sleep 20
            fi
          done
      
      - name: 📢 Display Application URL
        run: |
          echo "================================================"
          echo "🚀 All Services Deployed Successfully!"
          echo "================================================"
          echo "🌐 Access URL: ${{ steps.get-alb-url.outputs.alb_url }}"
          echo "📊 Services Status:"
          echo "   ✅ Frontend Service: Ready"
          echo "   ✅ Backend API Service: Ready" 
          echo "   ✅ Community Service: Ready"
          echo "   ✅ Database Configuration: Complete"
          echo "   ✅ Database Migrations: Complete"
          echo "🎉 Application is fully operational!"
          echo "================================================"

  rollback-check:
    name: "Rollback Check"
    runs-on: ubuntu-latest
    needs: [deploy-frontend, deploy-backend, deploy-community]
    environment: production
    if: always()

    steps:
      - name: 🔑 Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: ☸️ Update kubeconfig
        run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
      
      - name: 🔍 Check Deployment Status
        run: |
          echo "=== 最终部署状态检查 ==="
          kubectl get deployments -n ${{ env.NAMESPACE }}
          echo ""
          echo "=== Pod 状态 ==="
          kubectl get pods -n ${{ env.NAMESPACE }}
          echo ""
          echo "=== 服务状态 ==="
          kubectl get services -n ${{ env.NAMESPACE }}