name: Deploy Comic Website

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: comic-website-prod
  PROJECT_NAME: comic-website
  ENVIRONMENT: prod
  NAMESPACE: comic-website
  TF_BACKEND_BUCKET: comic-website-tfstate-2024

jobs:
  terraform-infrastructure:
    name: "Terraform - Create Infrastructure"
    runs-on: ubuntu-latest
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.5.0

    - name: Create S3 Backend Bucket
      run: |
        # Ê£ÄÊü• bucket ÊòØÂê¶Â∑≤Â≠òÂú®
        if aws s3 ls "s3://${{ env.TF_BACKEND_BUCKET }}" 2>/dev/null; then
          echo "‚úÖ S3 bucket already exists"
        else
          echo "üèóÔ∏è Creating S3 bucket for Terraform backend..."
          # us-east-1 Âå∫ÂüüÂàõÂª∫ bucket Êó∂‰∏çÊåáÂÆö LocationConstraint
          aws s3api create-bucket \
            --bucket ${{ env.TF_BACKEND_BUCKET }} \
            --region ${{ env.AWS_REGION }}
          
          echo "üîÑ Enabling versioning..."
          aws s3api put-bucket-versioning \
            --bucket ${{ env.TF_BACKEND_BUCKET }} \
            --versioning-configuration Status=Enabled
          
          echo "üîê Enabling encryption..."
          aws s3api put-bucket-encryption \
            --bucket ${{ env.TF_BACKEND_BUCKET }} \
            --server-side-encryption-configuration '{
              "Rules": [
                {
                  "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "AES256"
                  }
                }
              ]
            }'
        fi

    - name: Create DynamoDB Table for State Locking
      run: |
        # Ê£ÄÊü• DynamoDB Ë°®ÊòØÂê¶Â∑≤Â≠òÂú®
        if aws dynamodb describe-table --table-name comic-website-tfstate-lock --region ${{ env.AWS_REGION }} 2>/dev/null; then
          echo "‚úÖ DynamoDB table already exists"
        else
          echo "üèóÔ∏è Creating DynamoDB table for state locking..."
          aws dynamodb create-table \
            --table-name comic-website-tfstate-lock \
            --attribute-definitions AttributeName=LockID,AttributeType=S \
            --key-schema AttributeName=LockID,KeyType=HASH \
            --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 \
            --region ${{ env.AWS_REGION }}
          
          echo "‚è≥ Waiting for table to be active..."
          aws dynamodb wait table-exists \
            --table-name comic-website-tfstate-lock \
            --region ${{ env.AWS_REGION }}
        fi

    - name: Terraform Init
      id: init
      run: |
        cd terraform
        terraform init \
          -backend-config="bucket=${{ env.TF_BACKEND_BUCKET }}" \
          -backend-config="key=terraform/state/terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \
          -backend-config="encrypt=true" \
          -backend-config="dynamodb_table=comic-website-tfstate-lock"

    - name: Terraform Format
      id: fmt
      run: |
        cd terraform
        terraform fmt -check

    - name: Terraform Validate
      id: validate
      run: |
        cd terraform
        terraform validate

    - name: Terraform Plan
      id: plan
      run: |
        cd terraform
        terraform plan \
          -var="aws_region=${{ env.AWS_REGION }}" \
          -var="project_name=${{ env.PROJECT_NAME }}" \
          -var="environment=${{ env.ENVIRONMENT }}" \
          -var="db_password=${{ secrets.DB_PASSWORD }}" \
          -out=tfplan

    - name: Terraform Apply
      if: github.ref == 'refs/heads/main'
      run: |
        cd terraform
        terraform apply -auto-approve tfplan

    - name: Update kubeconfig
      if: github.ref == 'refs/heads/main'
      run: |
        cd terraform
        aws eks update-kubeconfig \
          --region ${{ env.AWS_REGION }} \
          --name ${{ env.CLUSTER_NAME }}

    - name: Setup Kubernetes tools
      if: github.ref == 'refs/heads/main'
      uses: azure/setup-kubectl@v3
      
    - name: Setup Helm
      if: github.ref == 'refs/heads/main'
      uses: azure/setup-helm@v3

    - name: Install ALB Ingress Controller with OIDC
      if: github.ref == 'refs/heads/main'
      run: |
        # Add EKS charts repository
        helm repo add eks https://aws.github.io/eks-charts
        helm repo update
        
        # Ëé∑Âèñ Terraform ËæìÂá∫
        cd terraform
        VPC_ID=$(terraform output -raw vpc_id)
        ALB_ROLE_ARN=$(terraform output -raw alb_controller_role_arn)
        cd ..
        
        echo "üîß Creating ALB Controller ServiceAccount with OIDC..."
        
        # ÂàõÂª∫ ServiceAccount ‰ΩøÁî® OIDC
        kubectl apply -f - <<EOF
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: aws-load-balancer-controller
          namespace: kube-system
          labels:
            app.kubernetes.io/name: aws-load-balancer-controller
            app.kubernetes.io/component: controller
          annotations:
            eks.amazonaws.com/role-arn: $ALB_ROLE_ARN
        EOF
        
        echo "üìù Creating ALB Controller values file..."
        
        # ÂàõÂª∫ Helm values Êñá‰ª∂
        cat > alb-values.yaml <<EOF
        clusterName: ${{ env.CLUSTER_NAME }}
        serviceAccount:
          create: false
          name: aws-load-balancer-controller
        region: ${{ env.AWS_REGION }}
        vpcId: $VPC_ID
        image:
          repository: 602401143452.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/amazon/aws-load-balancer-controller
        replicaCount: 2
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        podLabels:
          app.kubernetes.io/name: aws-load-balancer-controller
          app.kubernetes.io/component: controller
        nodeSelector:
          kubernetes.io/arch: amd64
        tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
        - effect: "NoSchedule"
          key: "node-role.kubernetes.io/master"
        - effect: "NoExecute"
          key: "node-role.kubernetes.io/control-plane"
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - aws-load-balancer-controller
                topologyKey: kubernetes.io/hostname
        EOF
        
        echo "üöÄ Installing AWS Load Balancer Controller..."
        helm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller \
          -n kube-system \
          -f alb-values.yaml \
          --version 1.6.1 \
          --wait
        
        echo "‚è≥ Waiting for ALB Controller pods to be ready..."
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system --timeout=300s
        
        echo "‚úÖ ALB Controller installed successfully with OIDC!"

    - name: Save Infrastructure Outputs
      if: github.ref == 'refs/heads/main'
      run: |
        cd terraform
        echo "DB_HOST=$(terraform output -raw rds_endpoint)" >> $GITHUB_ENV
        echo "S3_BUCKET_NAME=$(terraform output -raw s3_bucket_name)" >> $GITHUB_ENV
        echo "BACKEND_ECR_URL=$(terraform output -raw backend_ecr_repository_url)" >> $GITHUB_ENV
        echo "FRONTEND_ECR_URL=$(terraform output -raw frontend_ecr_repository_url)" >> $GITHUB_ENV

  deploy-applications:
    name: "Deploy Applications to EKS"
    runs-on: ubuntu-latest
    needs: terraform-infrastructure
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig \
          --region ${{ env.AWS_REGION }} \
          --name ${{ env.CLUSTER_NAME }}

    - name: Setup Kubernetes tools
      uses: azure/setup-kubectl@v3

    - name: Setup script permissions
      run: chmod +x scripts/*.sh

    - name: Get Terraform Outputs
      run: |
        cd terraform
        DB_HOST=$(terraform output -raw rds_endpoint)
        S3_BUCKET_NAME=$(terraform output -raw s3_bucket_name)
        echo "DB_HOST=$DB_HOST" >> $GITHUB_ENV
        echo "S3_BUCKET_NAME=$S3_BUCKET_NAME" >> $GITHUB_ENV

    - name: Build and Push Docker images
      run: |
        ./scripts/build-push-images.sh ${{ secrets.AWS_ACCOUNT_ID }}
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

    - name: Setup Kubernetes Secrets
      run: |
        ./scripts/setup-secrets.sh
      env:
        DB_HOST: ${{ env.DB_HOST }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        JWT_SECRET: ${{ secrets.JWT_SECRET }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        S3_BUCKET_NAME: ${{ env.S3_BUCKET_NAME }}

    - name: Deploy Applications
      run: |
        ./scripts/deploy-k8s.sh
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        DB_HOST: ${{ env.DB_HOST }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        JWT_SECRET: ${{ secrets.JWT_SECRET }}
        S3_BUCKET_NAME: ${{ env.S3_BUCKET_NAME }}

    - name: Verify Deployment
      run: |
        echo "üìä Final deployment status:"
        kubectl get deployments,services,ingress -n ${{ env.NAMESPACE }}
        
        echo "üîç Pods status:"
        kubectl get pods -n ${{ env.NAMESPACE }} -o wide
        
        echo "üîß ALB Controller status:"
        kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller

    - name: Health Check
      run: |
        # Á≠âÂæÖ ALB ÂàõÂª∫
        echo "‚è≥ Waiting for ALB to be created..."
        sleep 60
        
        ALB_URL=$(kubectl get ingress comic-website-ingress -n ${{ env.NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "pending")
        
        if [ "$ALB_URL" != "pending" ]; then
          echo "üåê ALB URL: http://$ALB_URL"
          echo "üîç Performing health check..."
          # ÈáçËØïÂá†Ê¨°ÂÅ•Â∫∑Ê£ÄÊü•
          for i in {1..5}; do
            if curl -f http://$ALB_URL/health; then
              echo "‚úÖ Health check passed!"
              break
            else
              echo "‚ö†Ô∏è Health check attempt $i failed, retrying in 10 seconds..."
              sleep 10
            fi
          done
        else
          echo "‚è∞ ALB is still being created, check AWS console for progress"
        fi